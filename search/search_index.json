{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to LLLM","text":"<p>LLLM is a lightweight framework designed for researchers to fast-prototype advanced agentic systems. It emphasizes minimalism, modularity, and type safety, with first-class support for program synthesis and neuro-symbolic capabilities.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Modular Architecture: Core abstractions, providers, tools, and memory are decoupled.</li> <li>Type Safety: Built on Pydantic for robust data validation and strict typing.</li> <li>Provider Interface: First-class OpenAI support with an extensible interface for adding more providers.</li> <li>Neuro-Symbolic Design: Advanced prompt management with structured output, exception handling, and interrupt logic.</li> <li>Jupyter Sandbox: Secure code execution environment for program synthesis.</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>pip install lllm\n</code></pre> <p>See Getting Started for more details.</p>"},{"location":"#experimental-features","title":"Experimental Features","text":"<ul> <li><code>lllm.tools.cua</code> (Computer Use Agent) integrates with Playwright and the OpenAI Computer Use API. Expect rapid iteration and breaking changes.</li> <li>Responses API routing is available per agent via <code>api_type = \"response\"</code>; today it only targets OpenAI models that advertise the requisite features.</li> </ul>"},{"location":"#work-in-progress","title":"Work in Progress","text":"<ul> <li>Additional providers (Anthropic, Gemini, local runtimes) are on the roadmap but not yet available in the <code>lllm.providers</code> registry.</li> <li>Provider-agnostic streaming and richer auto-discovery ergonomics (hot-reloading prompts/proxies) are actively being designed.</li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#installation","title":"Installation","text":"<p>Install LLLM using pip:</p> <pre><code>pip install lllm\n</code></pre>"},{"location":"getting-started/#basic-usage","title":"Basic Usage","text":"<p>Here is a simple example of how to create a chat agent:</p> <pre><code>from lllm import AgentBase, Prompt, register_prompt\n\n# 1. Define a Prompt\nsimple_prompt = Prompt(\n    path=\"simple_chat\",\n    prompt=\"You are a helpful assistant. User says: {user_input}\"\n)\nregister_prompt(simple_prompt)\n\n# 2. Define an Agent\nclass SimpleAgent(AgentBase):\n    agent_type = \"simple\"\n    agent_group = [\"assistant\"]\n\n    def call(self, task: str, **kwargs):\n        # Initialize dialog with user input\n        dialog = self.agents[\"assistant\"].init_dialog({\"user_input\": task})\n        # Call the agent\n        response, dialog, _ = self.agents[\"assistant\"].call(dialog)\n        return response.content\n\n# 3. Configure and Run\nconfig = {\n    \"name\": \"simple_chat_agent\",\n    \"log_dir\": \"./logs\",\n    \"log_type\": \"localfile\",\n    \"provider\": \"openai\",\n    \"auto_discover\": True,\n    \"agent_configs\": {\n        \"assistant\": {\n            \"model_name\": \"gpt-4o-mini\",\n            \"system_prompt_path\": \"simple_chat\",\n            \"temperature\": 0.7,\n        }\n    }\n}\n\nagent = SimpleAgent(config, ckpt_dir=\"./ckpt\")\nprint(agent(\"Hello!\"))\n</code></pre> <p>Set <code>provider</code> to any backend you register via <code>lllm.providers.register_provider</code>, and flip <code>auto_discover</code> to <code>False</code> if you want to opt out of scanning <code>lllm.toml</code> paths when instantiating agents or proxies.</p>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Agents</li> <li>Explore Prompts</li> <li>Check out Examples</li> </ul>"},{"location":"getting-started/#sandbox-smoke-test","title":"Sandbox Smoke Test","text":"<p>Want to verify that the Jupyter sandbox works in your environment without writing code? Run:</p> <pre><code>python examples/jupyter_sandbox_smoke.py --session demo\n</code></pre> <p>This creates a notebook under <code>.lllm_sandbox/demo/demo.ipynb</code> with a sample markdown and code cell. Open it in Jupyter or VS Code to inspect the generated notebook and build from there.</p>"},{"location":"getting-started/#sample-lllmtoml","title":"Sample <code>lllm.toml</code>","text":"<p>Auto-discovery is easiest to learn by copying the example config:</p> <pre><code>cp lllm.toml.example lllm.toml\n</code></pre> <p>The sample points to <code>examples/autodiscovery/prompts/</code> and <code>examples/autodiscovery/proxies/</code>. Edit those folders (or add new ones) to register your own prompts and proxies automatically on import.</p>"},{"location":"getting-started/#developer-test-suite","title":"Developer Test Suite","text":"<p>If you\u2019re contributing to LLLM itself, run the full suite before sending a PR:</p> <pre><code>pytest\n</code></pre> <p>This command covers unit tests, sandbox mocks, CLI/template smoke tests, and the recorded OpenAI tool-call flows. When you need to refresh the recordings, update the JSON files under <code>tests/integration/recordings/</code> and rerun the integration tests.</p>"},{"location":"advanced/neuro-symbolic/","title":"Neuro-Symbolic Workflows","text":"<p>LLLM\u2019s prompts and agent loop were designed so that researchers can combine symbolic constraints (XML tags, Markdown sections, structured parsers) with probabilistic reasoning. The recommended pattern is:</p> <ol> <li>Define schemas \u2013 create <code>Prompt</code> objects that declare XML/Markdown tags or attach a Pydantic <code>response_format</code> model.</li> <li>Enable handlers \u2013 customize <code>exception_prompt</code>/<code>interrupt_prompt</code> so the agent can repair malformed output or summarize tool results before continuing.</li> <li>Link tools \u2013 register <code>Function</code> objects that wrap symbolic routines (e.g., theorem checkers, graph search) and attach them to prompts via <code>functions_list</code>.</li> <li>Parse deterministically \u2013 leverage the <code>parser</code> callback on prompts to convert returned text into strongly typed Python objects.</li> </ol> <p>Because the agent loop keeps retrying until the parser succeeds (or the retry budget is exhausted), you can layer multiple symbolic constraints without losing the rapid iteration benefits of free-form LLM prompting.</p>"},{"location":"advanced/program-synthesis/","title":"Program Synthesis With LLLM","text":"<p>LLLM ships with a Jupyter-based sandbox and a (work-in-progress) Computer Use Agent so you can iteratively synthesize and execute code under tight supervision.</p> <ul> <li>Use <code>lllm.sandbox.JupyterSession</code> to spin up a reproducible notebook workspace. The inserted init cell exposes <code>CALL_API</code>, giving your prompts access to the same proxies/tools configured in <code>lllm.toml</code>.</li> <li>Capture tool output via prompt interrupt handlers so the agent can reason about code execution results before deciding whether to continue editing.</li> <li>For browser-driven or GUI-heavy workflows, the <code>lllm.tools.cua</code> module demonstrates how to normalize keyboard/mouse actions and surface screenshots back to the agent loop.</li> </ul> <p>Combine these components with structured prompts (see the neuro-symbolic guide) to build agents that can write, run, and validate code with minimal boilerplate.</p>"},{"location":"architecture/overview/","title":"Architecture Overview","text":"<p>LLLM ships as a Python package (<code>lllm/</code>) plus reusable templates (<code>template/</code>). The repository can be used directly as a library (install via <code>pip install git+...</code>) or cloned as a project starter.</p>"},{"location":"architecture/overview/#repository-layout","title":"Repository Layout","text":"Path Purpose <code>lllm/</code> Core runtime (agents, prompts, logging, proxies, sandbox, CLI helpers). <code>template/</code> Scaffolds consumed by <code>lllm create --name &lt;system&gt;</code>. Includes a minimal <code>init_template</code> and a richer <code>example</code> template with ready-made proxies. <code>README.md</code> Quick-start instructions for installing the package and enabling auto-discovery. <code>requirements.txt</code> / <code>pyproject.toml</code> Packaging metadata when installing as a library. <p>Inside <code>lllm/</code> the high-level structure is:</p> <ul> <li><code>llm.py</code> \u2013 agent orchestration, dialog state machine, and LLM caller/responder implementations.</li> <li><code>models.py</code> \u2013 dataclasses for <code>Prompt</code>, <code>Message</code>, <code>Function</code>, <code>FunctionCall</code>, and MCP tooling.</li> <li><code>proxies.py</code> \u2013 API proxy registry plus the <code>Proxy</code> runtime that normalizes third-party APIs.</li> <li><code>sandbox.py</code> \u2013 programmable Jupyter notebook sandbox for code-execution agents.</li> <li><code>config.py</code> &amp; <code>discovery.py</code> \u2013 <code>lllm.toml</code> resolution and auto-registration of prompts/proxies.</li> <li><code>log.py</code> \u2013 replayable logging backends (<code>LocalFileLog</code>, <code>NoLog</code>).</li> <li><code>const.py</code> \u2013 model metadata, pricing logic, enumerations, and helpers like tokenizers.</li> <li><code>utils.py</code> \u2013 filesystem helpers, caching, Streamlit-friendly wrappers, and API utilities.</li> <li><code>cli.py</code> \u2013 implementation of the <code>lllm</code> CLI entry-point.</li> </ul>"},{"location":"architecture/overview/#runtime-flow","title":"Runtime Flow","text":"<ol> <li>Configuration &amp; discovery \u2013 <code>lllm.auto_discover()</code> runs on import. It reads <code>lllm.toml</code> (or <code>$LLLM_CONFIG</code>) to find prompt and proxy folders, imports every module inside, and registers any <code>Prompt</code> or <code>BaseProxy</code> subclasses it encounters. Environment variables let you opt-out (<code>LLLM_AUTO_DISCOVER=0</code>).</li> <li>System bootstrap \u2013 A system (see <code>template/init_template/system/system.py</code>) constructs an agent via <code>lllm.llm.build_agent</code>, passing the YAML configuration, checkpoint directory, and a stream/logger implementation.</li> <li>Agent call loop \u2013 <code>Agent.call</code> seeds a dialog with its system prompt, loads an invocation prompt, and drives the deterministic agent-call state machine until it yields a parsed response or an exception.</li> <li>Prompt handlers \u2013 If the response contains errors, exception handlers rewrite the dialog and retry. If the response triggers function calls, interrupt handlers inject tool results and continue the loop.</li> <li>Proxy execution \u2013 When tool/function calls target external APIs, they go through the <code>Proxy</code> runtime. Proxy modules describe endpoints declaratively using decorators so that agents can enumerate available tools and call them uniformly.</li> <li>Sandboxed tooling \u2013 For advanced agents, the sandbox components provide Jupyter kernels and computer-use helpers (see <code>lllm/sandbox.py</code> and <code>lllm/tools/cua.py</code>). These enable notebook automation or browser automation with a standard interface.</li> <li>Logging &amp; replay \u2013 Every dialog, frontend event, and replayable artifact is written to a <code>ReplayableLogBase</code> collection. Implementations range from no-op logging (<code>NoLog</code>) to local file logging with per-session folders.</li> </ol>"},{"location":"architecture/overview/#component-relationships","title":"Component Relationships","text":"<ul> <li>Prompts depend on parsers, functions, and MCP servers defined in <code>lllm/models.py</code>.</li> <li>Agents consume prompts and proxies; agents are registered automatically via <code>AgentBase.__init_subclass__</code>.</li> <li>Proxies can be shared between systems, and their metadata powers auto-generated API catalogs that prompts can embed in instructions.</li> <li>Templates wire all of the above into runnable systems and document expected configuration keys.</li> </ul> <p>For a deep dive into the agent state machine, continue to Agent Call.</p>"},{"location":"core/agent-call/","title":"Agent Call","text":"<p>Agent calls are the defining concept of LLLM. Instead of exposing raw LLM completions, every agent implements a deterministic state machine that must transition from an initial dialog to a well-defined output state (or raise an error). This contract keeps downstream systems simple\u2014no consumer needs to guess whether the model \"felt done\".</p> <p></p>"},{"location":"core/agent-call/#llm-call-vs-agent-call","title":"LLM Call vs. Agent Call","text":"LLM Call Agent Call Input Flat list of chat messages. <code>Dialog</code> seeded with a top-level <code>Prompt</code>, plus optional prompt arguments. Output Raw model string plus metadata. Parsed response, updated dialog, and function-call trace. Responsibility Caller decides whether to retry, parse, or continue. Agent handles retries, parsing, exception recovery, and interrupts until it reaches the desired state. Determinism Best-effort. Guaranteed next state or explicit exception. <p>The <code>Agent</code> dataclass in <code>lllm/llm.py</code> exposes <code>call(dialog, extra, args, parser_args)</code> to run the loop. <code>AgentBase</code> (and <code>AsyncAgentBase</code>) wrap this call with logging (<code>StreamWrapper</code>) and a user-friendly <code>.call(task)</code> signature for system builders. Under the hood each agent delegates to a provider implementation (<code>lllm.providers.BaseProvider</code>) so the same loop can target OpenAI\u2019s Chat Completions API or the Responses API by toggling the <code>api_type</code> field on the agent configuration.</p>"},{"location":"core/agent-call/#state-machine-lifecycle","title":"State Machine Lifecycle","text":"<ol> <li>Seed dialog \u2013 <code>Agent.init_dialog</code> places the system prompt (usually pulled from <code>PROMPT_REGISTRY</code> via <code>Prompts('agent_root')</code>).</li> <li>Send invocation prompt \u2013 Systems typically <code>send_message</code> with a task-specific prompt (e.g., <code>prompts('task_query')</code>). The <code>Dialog.top_prompt</code> pointer remembers which prompt introduced the latest user turn.</li> <li>LLM call \u2013 The provider decides whether to hit <code>chat.completions</code> or the Responses API depending on <code>api_type</code> (<code>completion</code> vs. <code>response</code>) and prompt needs, attaches registered functions/MCP servers, and sends the dialog history.</li> <li>Response handling \u2013</li> <li>If the model returned tool calls, LLLM invokes each linked <code>Function</code>, appends a tool message generated by <code>Prompt.interrupt_handler</code>, and loops again.</li> <li>If the parser raised a <code>ParseError</code>, <code>Prompt.exception_handler</code> is invoked with the error message and the dialog is retried (up to <code>max_exception_retry</code>).</li> <li>Network/runtime issues trigger exponential backoff, optional \"LLM recall\" retries, and persist structured error reports.</li> <li>Completion \u2013 As soon as the model returns a valid assistant message without function calls, the loop terminates and the parsed payload plus the final dialog are returned.</li> </ol>"},{"location":"core/agent-call/#interrupt-vs-exception-handling","title":"Interrupt vs. Exception Handling","text":"<p>Each <code>Prompt</code> can specify inline handlers:</p> <ul> <li>Exception handler (<code>Prompt.exception_handler</code>) receives <code>{error_message}</code> whenever parsing or validation fails. These dialog branches are pruned after handling.</li> <li>Interrupt handler (<code>Prompt.interrupt_handler</code>) receives <code>{call_results}</code> after function execution. These branches remain in the dialog for transparency, and a final \"stop calling functions\" prompt ensures the agent produces a natural-language summary.</li> </ul> <p>Handlers inherit the prompt\u2019s parser, XML/MD tags, and allowed functions, so a single definition covers the entire agent loop.</p>"},{"location":"core/agent-call/#function-calls-mcp-and-tools","title":"Function Calls, MCP, and Tools","text":"<p><code>Prompt</code> instances can bundle:</p> <ul> <li><code>Function</code> objects (structured JSON schemas) that wrap Python callables.</li> <li>MCP server descriptors for OpenAI\u2019s Model Context Protocol tools.</li> <li>Optional <code>allow_web_search</code> and <code>computer_use_config</code> hints that enable OpenAI-native web search or computer-use toolchains when supported by the model card (<code>const.py</code>).</li> </ul> <p>During an agent call, every function call is tracked as a <code>FunctionCall</code> object with <code>arguments</code>, <code>result</code>, <code>result_str</code>, and <code>error_message</code>. The loop prevents duplicate calls by checking <code>FunctionCall.is_repeated</code>.</p>"},{"location":"core/agent-call/#selecting-the-llm-api","title":"Selecting the LLM API","text":"<p>Each agent entry in <code>agent_configs</code> accepts <code>api_type</code>:</p> <pre><code>[agent_configs.researcher]\nmodel_name = \"gpt-4o-mini\"\nsystem_prompt_path = \"research/system\"\napi_type = \"response\"  # or \"completion\"\n</code></pre> <ul> <li><code>completion</code> (default) uses Chat Completions. If <code>Prompt.format</code> is set, the provider automatically switches to <code>beta.chat.completions.parse</code> so you can keep using Pydantic response formats.</li> <li><code>response</code> opts into the OpenAI Responses API. When the target model advertises <code>web_search</code> or <code>computer_use</code> features, <code>Prompt.allow_web_search</code> and <code>Prompt.computer_use_config</code> materialize as native OpenAI tools. Responses API tool outputs are surfaced back through <code>Prompt.interrupt_handler</code> as <code>Roles.USER</code> messages so the assistant can summarize the tool transcript.</li> </ul>"},{"location":"core/agent-call/#classification-helpers","title":"Classification Helpers","text":"<p>The same machinery powers lightweight classifiers (<code>Agent.classify</code>, <code>Agent.binary_classify</code>). These helpers:</p> <ol> <li>Re-seed the dialog with a classifier prompt.</li> <li>Ask the model to emit a single token drawn from the provided class list.</li> <li>Inspect logprobs to return calibrated probabilities.</li> </ol> <p>Because classification still runs through the agent-call loop, you retain exception handling, retries, and structured logging.</p>"},{"location":"core/agent-call/#implementing-custom-agents","title":"Implementing Custom Agents","text":"<p>To ship a new agent:</p> <ol> <li>Subclass <code>AgentBase</code>, set <code>agent_type</code> and <code>agent_group</code> (e.g., <code>['researcher', 'editor']</code>).</li> <li>Within <code>__init__</code>, pick prompts via <code>Prompts('&lt;root&gt;')</code> and designate which sub-agent handles each task.</li> <li>Implement <code>call(self, task, **kwargs)</code> to orchestrate dialogs (see <code>template/example/system/agent/agent.py</code>).</li> </ol> <p>Agents register themselves automatically through <code>AgentBase.__init_subclass__</code>, so once your class is imported it becomes available to <code>build_agent</code> and the templates.</p>"},{"location":"core/agents/","title":"Agents","text":"<p>Agents are the core actors in the LLLM framework. They encapsulate the logic for interacting with LLMs, managing state (Dialog), and executing tools.</p>"},{"location":"core/agents/#the-agent-class","title":"The <code>Agent</code> Class","text":"<p>The <code>Agent</code> class represents a single LLM entity with a specific role, system prompt, and model configuration.</p>"},{"location":"core/agents/#key-attributes","title":"Key Attributes","text":"<ul> <li><code>name</code>: The name or role of the agent (e.g., \"assistant\", \"coder\").</li> <li><code>system_prompt</code>: The <code>Prompt</code> object defining the agent's persona and capabilities.</li> <li><code>model</code>: The LLM model to use (e.g., \"gpt-4o\").</li> <li><code>llm_provider</code>: The provider instance (e.g., <code>OpenAIProvider</code>).</li> </ul>"},{"location":"core/agents/#the-agentbase-class","title":"The <code>AgentBase</code> Class","text":"<p><code>AgentBase</code> is the base class for building complex agentic systems. It allows you to compose multiple <code>Agent</code> instances and define custom orchestration logic.</p>"},{"location":"core/agents/#creating-a-custom-agent","title":"Creating a Custom Agent","text":"<p>To create a custom agent, subclass <code>AgentBase</code> and implement the <code>call</code> method:</p> <pre><code>from lllm import AgentBase\n\nclass MyAgent(AgentBase):\n    agent_type = \"my_agent\"\n    agent_group = [\"worker\", \"reviewer\"] # Define sub-agents\n\n    def call(self, task: str, **kwargs):\n        # Implement your logic here\n        pass\n</code></pre>"},{"location":"core/discovery/","title":"Configuration &amp; Discovery","text":"<p>LLLM favors composition over hard-coded imports. Auto-discovery plus lightweight YAML allows an entire system to be wired together without editing Python entry points.</p>"},{"location":"core/discovery/#configuration-files","title":"Configuration Files","text":"<ol> <li><code>config/&lt;system&gt;/default.yaml</code> \u2013 runtime settings consumed by agents and systems.</li> <li><code>name</code>, <code>log_type</code>, <code>log_dir</code>, <code>ckpt_dir</code>, randomness controls.</li> <li><code>agent_configs</code> describing each model (name, prompt path, temperature, <code>max_completion_tokens</code>, optional <code>api_type</code>).</li> <li>Execution guards such as <code>max_exception_retry</code>, <code>max_interrupt_times</code>, <code>max_llm_recall</code>.</li> <li>Proxy activation and deploy toggles.</li> <li><code>lllm.toml</code> \u2013 discovery manifest read by <code>lllm.config.find_config_file</code> and <code>lllm.discovery.auto_discover</code>.</li> </ol> <p>Example (<code>template/lllm.toml</code>):</p> <pre><code>[prompts]\nfolders = [\"system/agent/prompts\"]\n\n[proxies]\nfolders = [\"system/proxy/modules\"]\n</code></pre> <p>Place this file at the project root or point to it via <code>$LLLM_CONFIG</code>. The loader searches parents recursively, so you can run tools from subdirectories without losing context.</p>"},{"location":"core/discovery/#environment-variables","title":"Environment Variables","text":"Variable Purpose <code>LLLM_CONFIG</code> Absolute path to a config file or folder; overrides auto-detection. <code>LLLM_AUTO_DISCOVER</code> Set to <code>0</code>, <code>false</code>, or <code>no</code> to skip auto-discovery (manual registration only). <code>TMP_DIR</code> Overrides the default temp/cache directory used by utils and error logging."},{"location":"core/discovery/#auto-discovery-workflow","title":"Auto-Discovery Workflow","text":"<p><code>lllm.auto_discover()</code> runs when the package is imported:</p> <ol> <li>Resolve the config path (<code>LLLM_CONFIG</code>, explicit argument, or nearest <code>lllm.toml</code>).</li> <li>Load TOML, collect prompt/proxy folders relative to the file.</li> <li>Import every <code>.py</code> file under those folders (excluding <code>__init__</code> and private files).</li> <li>Register each <code>Prompt</code> (keyed by <code>namespace/module.prompt.path</code>).</li> <li>Register each <code>BaseProxy</code> subclass (keyed by <code>_proxy_path</code> or <code>&lt;namespace&gt;/&lt;class&gt;</code>).</li> </ol> <p>Because registration happens at import time, simply adding a new prompt module to the folder makes it available across the repo without touching central registries.</p>"},{"location":"core/discovery/#yaml-tips","title":"YAML Tips","text":"<ul> <li>Keep secrets (API keys) out of the YAML and load them via environment variables inside your system/agent code.</li> <li>Use multiple YAML files (e.g., <code>config/prod.yaml</code>) and load the desired profile before building a system.</li> <li>Version-control template configs but store user-specific overrides elsewhere; the CLI scaffold already sets up a namespaced config folder.</li> </ul>"},{"location":"core/discovery/#disabling-discovery","title":"Disabling Discovery","text":"<p>When packaging LLLM as a reusable library, you may want to opt out of auto-imports. Set <code>LLLM_AUTO_DISCOVER=0</code>, then register prompts and proxies manually:</p> <pre><code>from lllm import register_prompt, register_proxy\nregister_prompt(my_prompt)\nregister_proxy(\"custom/my_proxy\", MyProxy)\n</code></pre> <p>This pattern is useful for unit tests or environments where dynamic imports are restricted.</p>"},{"location":"core/logging/","title":"Logging &amp; Observability","text":"<p>Agents are only as good as their observability. LLLM includes a replayable logging stack so every message, function call, and frontend event can be audited after the fact.</p>"},{"location":"core/logging/#collections-sessions","title":"Collections &amp; Sessions","text":"<p><code>lllm.const.RCollections</code> defines three canonical collections:</p> <ul> <li><code>dialogs</code> \u2013 one entry per dialog id created during a session.</li> <li><code>messages</code> \u2013 the full transcript for each dialog (<code>session/dialog_id</code>).</li> <li><code>frontend</code> \u2013 arbitrary UI or status events (used by <code>StreamWrapper</code>).</li> </ul> <p>A <code>Dialog</code> automatically logs every appended <code>Message</code> along with metadata (creator, role, cost, parsed payload). Forked dialogs inherit the log base so their lineage is tracked.</p>"},{"location":"core/logging/#log-base-implementations","title":"Log Base Implementations","text":"<p><code>lllm/log.py</code> provides a pluggable abstraction:</p> <ul> <li><code>ReplayableLogBase</code> (abstract) \u2013 enforces collection names and powers <code>ReplaySession.activities</code> for chronological replays.</li> <li><code>LocalFileLog</code> \u2013 stores JSON blobs on disk under <code>&lt;log_dir&gt;/&lt;collection&gt;/&lt;session&gt;/&lt;timestamp&gt;.json</code>.</li> <li><code>NoLog</code> \u2013 disables persistence while keeping the API surface identical.</li> </ul> <p><code>build_log_base(config)</code> chooses an implementation based on the YAML configuration (<code>log_type: localfile | none</code>). Because the log base is passed to every agent, switching persistence strategies requires no code changes.</p>"},{"location":"core/logging/#stream-frontend-helpers","title":"Stream &amp; Frontend Helpers","text":"<p><code>lllm.utils.PrintSystem</code> and <code>StreamWrapper</code> mirror a minimal Streamlit-like interface:</p> <ul> <li><code>.write</code>, <code>.markdown</code>, <code>.spinner</code>, <code>.expander</code>, <code>.divider</code>, <code>.code</code> etc. log frontend events into the <code>frontend</code> collection.</li> <li>Agents call <code>self.set_st(session_name)</code> before each task to associate logs with a human-readable session id.</li> </ul> <p>When building custom UIs, implement the same methods and forward them to your logging or telemetry stack.</p>"},{"location":"core/logging/#replay-workflows","title":"Replay Workflows","text":"<p><code>ReplaySession</code> stitches dialogs and frontend entries into a time-sorted list of <code>Activity</code> objects. This enables:</p> <ul> <li>Rich transcripts (who said what, which prompt triggered the response, which tool ran).</li> <li>Time-based debugging (spot retries, rate-limit pauses, etc.).</li> <li>Lightweight analytics (count function calls per agent, measure exception rates).</li> </ul> <p>Because every <code>Message</code> stores <code>usage</code> and <code>CompletionCost</code>, you can compute spend per run without instrumenting each API call manually.</p>"},{"location":"core/logging/#configuration-tips","title":"Configuration Tips","text":"<ul> <li>Keep <code>log_dir</code> outside version control (default <code>./logs</code>).</li> <li>Set <code>log_type: none</code> for unit tests to avoid filesystem noise.</li> <li>For long-running systems, use a persistent volume and prune old sessions by deleting directories under <code>logs/</code>.</li> <li>Combine logging with sandbox metadata: <code>JupyterSession.to_dict()</code> captures proxy activation and cutoff dates so a replay can rebuild the exact environment.</li> </ul>"},{"location":"core/prompts/","title":"Prompts","text":"<p>In LLLM, a <code>Prompt</code> is more than just a text string. It is a structured object that defines:</p> <ul> <li>The template string (with variables like <code>{user_input}</code>).</li> <li>Available functions/tools.</li> <li>Exception handling logic.</li> <li>Interrupt handling logic (for tool calls).</li> <li>Output parsing and formatting.</li> </ul>"},{"location":"core/prompts/#defining-a-prompt","title":"Defining a Prompt","text":"<pre><code>from lllm import Prompt, Function\n\n# Define a tool\ndef get_weather(location: str):\n    return \"Sunny\"\n\nweather_tool = Function(name=\"get_weather\", ...)\nweather_tool.link_function(get_weather)\n\n# Define the prompt\nmy_prompt = Prompt(\n    path=\"weather_bot\",\n    prompt=\"You are a weather bot. User asks: {question}\",\n    functions_list=[weather_tool],\n    exception_prompt=\"I encountered an error: {error_message}. Let me try again.\",\n)\n</code></pre>"},{"location":"core/prompts/#neuro-symbolic-features","title":"Neuro-Symbolic Features","text":"<p>Prompts support advanced features like recursive exception handling and structured output enforcement, enabling robust neuro-symbolic workflows.</p>"},{"location":"core/providers/","title":"Providers","text":"<p>Providers abstract the underlying LLM APIs, allowing you to switch between models and services easily.</p>"},{"location":"core/providers/#baseprovider","title":"<code>BaseProvider</code>","text":"<p>All providers inherit from <code>BaseProvider</code>. This abstract base class enforces a consistent interface for:</p> <ul> <li><code>call</code>: Executing a completion request.</li> <li><code>cost</code>: Calculating the cost of a request.</li> </ul>"},{"location":"core/providers/#supported-providers","title":"Supported Providers","text":""},{"location":"core/providers/#openai","title":"OpenAI","text":"<p>The <code>OpenAIProvider</code> supports all OpenAI-compatible APIs (including Together AI, etc.).</p> <pre><code>from lllm.providers.openai import OpenAIProvider\n\nprovider = OpenAIProvider(config)\n</code></pre>"},{"location":"core/providers/#adding-a-new-provider","title":"Adding a New Provider","text":"<p>To add a new provider (e.g., Anthropic), subclass <code>BaseProvider</code> and implement the required methods.</p>"},{"location":"core/proxy-and-sandbox/","title":"Proxies &amp; Sandbox","text":"<p>LLLM treats tool access as a first-class capability. Proxies standardize how agents reach external APIs, while the sandbox module enables stateful notebook or browser-style execution. Together they make it possible to build advanced coding agents that mix reasoning, API calls, and code execution.</p>"},{"location":"core/proxy-and-sandbox/#baseproxy-endpoint-registration","title":"BaseProxy &amp; Endpoint Registration","text":"<p><code>lllm/proxies.py</code> defines <code>BaseProxy</code>, a reflection-based registry for HTTP endpoints. Endpoints are declared via decorators:</p> <pre><code>from lllm.proxies import BaseProxy, ProxyRegistrator\n\n@ProxyRegistrator(path=\"finance/fmp\", name=\"Financial Modeling Prep\", description=\"Market data API\")\nclass FMPProxy(BaseProxy):\n    base_url = \"https://financialmodelingprep.com/api/v3\"\n    api_key_name = \"apikey\"\n\n    @BaseProxy.endpoint(\n        category=\"search\",\n        endpoint=\"search-symbol\",\n        description=\"Search tickers\",\n        params={\"query*\": (str, \"AAPL\"), \"limit\": (int, 10)},\n        response=[{\"symbol\": \"AAPL\", \"name\": \"Apple\"}],\n    )\n    def search_symbol(self, params):\n        return params\n</code></pre> <p>At import time, decorated methods receive metadata (<code>endpoint_info</code>). Auto-discovery (based on <code>lllm.toml</code>) imports every module listed under the <code>[proxies]</code> section so the <code>ProxyRegistrator</code> decorator can register the class. Helpers such as <code>endpoint_directory</code>, <code>api_directory</code>, and <code>auto_test()</code> can then use the stored metadata to build instructions for the model or smoke-test endpoints.</p>"},{"location":"core/proxy-and-sandbox/#runtime-proxy","title":"Runtime Proxy","text":"<p>The <code>Proxy</code> runtime composes multiple <code>BaseProxy</code> subclasses:</p> <pre><code>from lllm.proxies import Proxy\n\nproxy = Proxy(activate_proxies=[\"finance/fmp\", \"news/gkg\"], cutoff_date=\"2024-01-01\")\nresult = proxy(\"finance/fmp/search/search-symbol\", {\"query\": \"AAPL\"})\n</code></pre> <p>Features:</p> <ul> <li>Selective activation \u2013 <code>activate_proxies</code> filters which proxies load; missing proxies are skipped without crashing the agent.</li> <li>Cutoff dates \u2013 Pass <code>cutoff_date</code> to enforce data availability constraints. Each proxy can opt-in to date filtering via <code>dt_cutoff</code> metadata.</li> <li>Deployment mode \u2013 <code>deploy_mode=True</code> disables cutoffs for production runs.</li> <li>Documentation helpers \u2013 <code>proxy.api_catalog</code>, <code>proxy.api_directory</code>, and <code>proxy.retrieve_api_docs()</code> (available on richer proxy implementations) return rich text that can be inserted into prompts so the model understands available tooling.</li> <li>Manual imports \u2013 When using <code>Proxy()</code> in isolation (e.g., inside a notebook), make sure you import the proxy modules you want first so <code>ProxyRegistrator</code> has a chance to register them. Call <code>from lllm.proxies import load_builtin_proxies; load_builtin_proxies()</code> to bring in the packaged proxies, or import your own modules explicitly.</li> <li>Inventory \u2013 Use <code>Proxy.available()</code> to inspect which proxy identifiers are currently loaded.</li> </ul> <p>The example template (<code>template/example/system/proxy/modules/*.py</code>) showcases large sets of proxies (finance, macro, search, Wolfram Alpha, etc.) managed through this infrastructure.</p>"},{"location":"core/proxy-and-sandbox/#prompt-integration","title":"Prompt Integration","text":"<p>Prompts can link directly to proxy-backed functions. A typical pattern:</p> <ol> <li>The proxy exposes a python method (e.g., <code>search_symbol</code>).</li> <li>A prompt defines a <code>Function</code> with the same signature.</li> <li><code>Prompt.link_function(\"search_symbol\", proxy_instance.search_symbol)</code> wires the tool call.</li> <li>The agent instructs the model to call <code>search_symbol</code> via function-calling. Tool responses are fed back through interrupt handlers, keeping the agent deterministic.</li> </ol> <p>Because proxies carry detailed metadata, you can also programmatically generate tool-selection prompts (\u201cHere\u2019s the API catalog... choose the minimal endpoint\u201d).</p>"},{"location":"core/proxy-and-sandbox/#sandbox-for-notebook-agents","title":"Sandbox for Notebook Agents","text":"<p><code>lllm/sandbox.py</code> implements a programmable Jupyter environment aimed at coding agents:</p> <ul> <li><code>JupyterSession</code> launches kernels, keeps notebooks on disk, and inserts a guarded init cell that imports project code plus the proxy runtime.</li> <li>Sessions store metadata such as proxy activation, cutoff dates, and project roots so that generated notebooks remain reproducible.</li> <li>Helpers like <code>insert_cell</code>, <code>overwrite_cell</code>, <code>run_all_cells</code>, and <code>directory_tree</code> let an agent (or a supervising process) manipulate the notebook without manual intervention.</li> <li>Execution is tracked with timestamps and can be synchronized with the logging subsystem.</li> </ul> <p>This sandbox doubles as the runtime for notebook-style proxies\u2014agents can spin up a session, run code, and feed the results back through interrupt handlers.</p>"},{"location":"core/proxy-and-sandbox/#computer-use-agent-cua","title":"Computer-Use Agent (CUA)","text":"<p>For browser-centric automation, <code>lllm/tools/cua.py</code> provides a stub Computer Use Agent powered by Playwright. It:</p> <ul> <li>Normalizes keyboard/mouse actions, scrolls, and text entry.</li> <li>Captures screenshots (with caching on failure) and enforces display boundaries.</li> <li>Defines control signals (e.g., <code>Ctrl+W</code>) so the model can terminate sessions safely.</li> <li>Illustrates how to extend LLLM with long-running, tool-controlled workflows.</li> </ul> <p>Although the CUA is optional, it demonstrates how proxies and sandboxed environments align: prompts produce tool calls, proxies/sandboxes execute them, and the agent ensures the conversation reaches a conclusion.</p>"},{"location":"guides/building-agents/","title":"Guide: Building an Agent","text":"<p>This walkthrough explains how to assemble a fully working agent using the components shipped in this repository.</p>"},{"location":"guides/building-agents/#1-bootstrap-a-system","title":"1. Bootstrap a System","text":"<p>Use the CLI scaffold or copy the example system.</p> <pre><code>lllm create --name research_system\ncd research_system\n</code></pre> <p>The scaffold creates:</p> <ul> <li><code>lllm.toml</code> with prompt/proxy folders.</li> <li><code>config/&lt;project&gt;/default.yaml</code> \u2013 edit this first (model name, logging, retries).</li> <li><code>system/agent/</code> and <code>system/proxy/</code> skeletons.</li> </ul>"},{"location":"guides/building-agents/#2-define-prompts","title":"2. Define Prompts","text":"<p>In <code>system/agent/prompts</code>, create prompt modules that describe your agent\u2019s behavior.</p> <pre><code># system/agent/prompts/researcher.py\nfrom lllm.models import Prompt\n\nanalysis = Prompt(\n    path=\"analysis\",\n    prompt=\"\"\"\n    You are a researcher...\n    &lt;analysis&gt;{task}&lt;/analysis&gt;\n    &lt;answer&gt;&lt;/answer&gt;\n    \"\"\",\n    xml_tags=[\"analysis\", \"answer\"],\n)\n</code></pre> <p>Prompts register automatically via auto-discovery, so the agent can load them with <code>Prompts('researcher')('analysis')</code>.</p>"},{"location":"guides/building-agents/#3-wire-tooling-via-proxies","title":"3. Wire Tooling via Proxies","text":"<p>Implement or reuse proxies under <code>system/proxy/modules</code>.</p> <pre><code>from lllm.proxies import BaseProxy, ProxyRegistrator\n\n@ProxyRegistrator(path=\"research/web\", name=\"Search\", description=\"Web search API\")\nclass SearchProxy(BaseProxy):\n    base_url = \"https://api.example.com\"\n    api_key_name = \"apikey\"\n\n    @BaseProxy.endpoint(...)\n    def search(self, params):\n        return params\n</code></pre> <p>Add the proxy folder to <code>lllm.toml</code>. Update your prompt to define <code>Function</code> objects and <code>link_function</code> to proxy methods so the agent can call them.</p>"},{"location":"guides/building-agents/#4-implement-the-agent","title":"4. Implement the Agent","text":"<p>Subclass <code>AgentBase</code> (or reuse the example <code>Vanilla</code> agent) under <code>system/agent/agent.py</code>.</p> <pre><code>from lllm.llm import AgentBase, Prompts\n\nclass ResearchAgent(AgentBase):\n    agent_type = \"research\"\n    agent_group = [\"research\"]\n\n    def __init__(self, config, ckpt_dir, stream, **kwargs):\n        super().__init__(config, ckpt_dir, stream)\n        self.agent = self.agents[\"research\"]\n        self.prompts = Prompts(\"research\")\n\n    def call(self, task: str, **kwargs):\n        dialog = self.agent.init_dialog()\n        dialog.send_message(self.prompts(\"analysis\"), {\"task\": task})\n        response, dialog, _ = self.agent.call(dialog)\n        return response.parsed\n</code></pre> <p>Multiple sub-agents can be grouped by listing more names in <code>agent_group</code> and referencing them via <code>self.agents['name']</code>.</p>"},{"location":"guides/building-agents/#5-build-the-system-wrapper","title":"5. Build the System Wrapper","text":"<p><code>system/system.py</code> wires configuration, checkpoint directories, and stream/logging utilities.</p> <pre><code>from system.agent.agent import build_agent\n\nclass ResearchSystem:\n    def __init__(self, config, stream=None):\n        self.agent = build_agent(config, config['ckpt_dir'], stream)\n\n    def call(self, task, **kwargs):\n        return self.agent(task, **kwargs)\n</code></pre>"},{"location":"guides/building-agents/#6-run-iterate","title":"6. Run &amp; Iterate","text":"<pre><code>from system.system import ResearchSystem\nfrom system.utils import load_config\n\nconfig = load_config(\"config/research/default.yaml\")\nsystem = ResearchSystem(config)\nprint(system.call(\"Summarize this paper\"))\n</code></pre> <ul> <li>Increase <code>max_exception_retry</code> when working with brittle parsers.</li> <li>Use <code>Proxy().api_catalog</code> to generate tool-selection prompts.</li> <li>Toggle <code>log_type</code> to <code>localfile</code> while debugging so you can replay entire sessions under <code>logs/</code>.</li> <li>For coding agents, spin up a <code>JupyterSession</code> via <code>lllm.sandbox</code> and surface session metadata inside your prompts so the model knows where files live.</li> </ul>"},{"location":"guides/project-template/","title":"Guide: Templates &amp; CLI","text":"<p>The <code>lllm</code> CLI bootstraps reproducible agent projects using the templates shipped in this repository.</p>"},{"location":"guides/project-template/#cli-overview","title":"CLI Overview","text":"<pre><code>pip install git+https://github.com/ChengJunyan1/lllm.git\nlllm --help\nlllm create --name system            # uses template/init_template\nlllm create --name lab --template example\n</code></pre> <p><code>lllm create</code> copies the chosen template into the current directory, performing simple text substitutions:</p> <ul> <li><code>__project_name__</code> \u2192 provided <code>--name</code>.</li> <li><code>{{project_name}}</code> and <code>{{PROJECT_NAME}}</code> placeholders are replaced in file contents and paths.</li> </ul> <p>Text files (<code>.py</code>, <code>.md</code>, <code>.toml</code>, <code>.yaml</code>, etc.) are rendered; binaries are copied verbatim.</p>"},{"location":"guides/project-template/#templates","title":"Templates","text":""},{"location":"guides/project-template/#init_template","title":"<code>init_template</code>","text":"<p>A minimal scaffold intended for fresh projects:</p> <ul> <li><code>lllm.toml</code> \u2013 points to <code>system/agent/prompts</code> and <code>system/proxy/modules</code>.</li> <li><code>config/&lt;project&gt;/default.yaml</code> \u2013 stub configuration with logging, agent, and proxy settings.</li> <li><code>system/agent/agent.py</code> \u2013 contains a <code>Vanilla</code> agent wired to <code>Prompts('vanilla')</code>.</li> <li><code>system/system.py</code> \u2013 exposes <code>SimpleSystem</code> with <code>.call()</code>.</li> </ul> <p>Use this template when you want a clean slate without bundled proxies or prompts.</p>"},{"location":"guides/project-template/#example","title":"<code>example</code>","text":"<p>A richer showcase that demonstrates:</p> <ul> <li>Multiple proxy modules (finance, macro, knowledge-base, Wolfram Alpha, custom MCP).</li> <li>Prompt definitions with Pydantic formats (<code>template/example/system/agent/prompts/vanilla.py</code>).</li> <li><code>system/system.py</code> illustrating how to manage experiment ids, streaming output, and async agents.</li> </ul> <p>This template also includes configuration defaults for cutting-edge models (e.g., <code>o4-mini-2025-04-16</code>).</p>"},{"location":"guides/project-template/#customizing-the-scaffold","title":"Customizing the Scaffold","text":"<ol> <li>Update configuration \u2013 edit <code>config/&lt;name&gt;/default.yaml</code>. Key sections:</li> <li><code>agent_configs</code> \u2013 specify <code>model_name</code>, <code>system_prompt_path</code>, <code>temperature</code>, etc.</li> <li>Retry and safety knobs (<code>max_exception_retry</code>, <code>max_interrupt_times</code>).</li> <li><code>activate_proxies</code> \u2013 list of proxy identifiers to preload (matching <code>_proxy_path</code>).</li> <li>Register prompts \u2013 add <code>.py</code> files under <code>system/agent/prompts</code>. Auto-discovery registers any module-level <code>Prompt</code> objects.</li> <li>Add proxies \u2013 drop new proxy modules under <code>system/proxy/modules</code>, decorate classes with <code>@ProxyRegistrator</code>, and configure API keys via environment variables.</li> <li>Version control \u2013 templates avoid user-specific secrets by design. Store credentials as env vars or encrypted secrets.</li> </ol>"},{"location":"guides/project-template/#publishing-docs-alongside-your-system","title":"Publishing Docs Alongside Your System","text":"<p>This repository now includes a MkDocs configuration (see <code>mkdocs.yml</code>). Projects generated from the template can copy the <code>docs/</code> structure or point to this repository as a submodule, then run <code>mkdocs build</code> to publish GitHub Pages documentation for their custom agents.</p>"},{"location":"guides/project-template/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>\"Path already exists\" \u2013 the CLI refuses to overwrite existing folders; pick a new <code>--name</code> or delete the directory manually.</li> <li>Auto-discovery misses modules \u2013 ensure <code>lllm.toml</code> paths are correct relative to the project root and that files end with <code>.py</code> (not <code>._py</code>).</li> <li>Proxy modules raise import errors \u2013 verify third-party dependencies (e.g., <code>requests</code>, <code>tqdm</code>) are installed in your environment.</li> </ul>"},{"location":"reference/core/","title":"Core API Reference","text":""},{"location":"reference/core/#agent","title":"Agent","text":""},{"location":"reference/core/#lllm.core.agent.Agent","title":"<code>lllm.core.agent.Agent</code>  <code>dataclass</code>","text":"Source code in <code>lllm/core/agent.py</code> <pre><code>@dataclass\nclass Agent:\n    name: str # the role of the agent, or a name of the agent\n    system_prompt: Prompt\n    model: str # the latest snapshot of a model\n    llm_provider: BaseProvider\n    log_base: ReplayableLogBase   \n    api_type: APITypes = APITypes.COMPLETION\n    model_args: Dict[str, Any] = field(default_factory=dict) # additional args, like temperature, seed, etc.\n    max_exception_retry: int = 3\n    max_interrupt_times: int = 5\n    max_llm_recall: int = 0\n\n    \"\"\"\n    Represents a single LLM agent with a specific role and capabilities.\n\n    Attributes:\n        name (str): The name or role of the agent (e.g., 'assistant', 'coder').\n        system_prompt (Prompt): The system prompt defining the agent's persona.\n        model (str): The model identifier (e.g., 'gpt-4o').\n        llm_provider (BaseProvider): The provider instance for LLM calls.\n        log_base (ReplayableLogBase): Logger for recording interactions.\n        model_args (Dict[str, Any]): Additional model arguments (temp, top_p, etc.).\n        max_exception_retry (int): Max retries for agent exceptions.\n        max_interrupt_times (int): Max consecutive tool call interrupts.\n        max_llm_recall (int): Max retries for LLM API errors.\n    \"\"\"\n\n    def __post_init__(self):\n        self.model_card = find_model_card(self.model)\n        self.model_card.check_args(self.model_args)\n        self.model = self.model_card.latest_snapshot.name\n\n    def reload_system(self, system_prompt: Prompt):\n        self.system_prompt = system_prompt\n\n    # initialize the dialog\n    def init_dialog(self, prompt_args: Optional[Dict[str, Any]] = None, session_name: str = None) -&gt; Dialog:\n        prompt_args = dict(prompt_args) if prompt_args else {}\n        if session_name is None:\n            session_name = dt.datetime.now().strftime('%Y%m%d_%H%M%S')+'_'+str(uuid.uuid4())[:6]\n        system_message = Message(\n            role=Roles.SYSTEM,\n            content=self.system_prompt(**prompt_args),\n            creator='system',\n        )\n        return Dialog(\n            [system_message],\n            self.log_base,\n            session_name,\n            top_prompt=self.system_prompt,\n        )\n\n    # send a message to the dialog manually\n    def send_message(\n        self,\n        dialog: Dialog,\n        prompt: Prompt,\n        prompt_args: Optional[Dict[str, Any]] = None,\n        creator: str = 'internal',\n        extra: Optional[Dict[str, Any]] = None,\n        role: Roles = Roles.USER,\n    ):\n        prompt_payload = dict(prompt_args) if prompt_args else None\n        extra_payload = dict(extra) if extra else None\n        return dialog.send_message(prompt, prompt_payload, creator=creator, extra=extra_payload, role=role)\n\n    # it performs the \"Agent Call\"\n    def call(\n        self,\n        dialog: Dialog,  # it assumes the prompt is already loaded into the dialog as the top prompt by send_message\n        extra: Optional[Dict[str, Any]] = None,  # for tracking additional information, such as frontend replay info\n        args: Optional[Dict[str, Any]] = None,  # for tracking additional information, such as frontend replay info\n        parser_args: Optional[Dict[str, Any]] = None,\n    ) -&gt; Tuple[Message, Dialog, List[FunctionCall]]:\n        \"\"\"\n        Executes the agent loop, handling LLM calls, tool execution, and interrupts.\n\n        Args:\n            dialog (Dialog): The current dialog state.\n            extra (Dict[str, Any], optional): Extra metadata for the call.\n            args (Dict[str, Any], optional): Additional arguments for the prompt.\n            parser_args (Dict[str, Any], optional): Arguments for the output parser.\n\n        Returns:\n            Tuple[Message, Dialog, List[FunctionCall]]: The final response message, the updated dialog, and a list of executed function calls.\n\n        Raises:\n            ValueError: If the agent fails to produce a valid response after retries.\n        \"\"\"\n        extra = dict(extra) if extra else {}\n        args = dict(args) if args else {}\n        parser_args = dict(parser_args) if parser_args else {}\n        # Prompt: a function maps prompt args and dialog into the expected output \n        current_prompt = dialog.top_prompt or self.system_prompt\n        interrupts = []\n        for i in range(10000 if self.max_interrupt_times == 0 else self.max_interrupt_times+1): # +1 for the final response\n            llm_recall = self.max_llm_recall \n            exception_retry = self.max_exception_retry \n            working_dialog = dialog.fork() # make a copy of the dialog, truncate all excception handling dialogs\n            while True: # ensure the response is no exception\n                execution_attempts = []\n                try:\n                    _model_args = self.model_args.copy()\n                    _model_args.update(args)\n\n                    response = self.llm_provider.call(\n                        working_dialog,\n                        current_prompt,\n                        self.model,\n                        _model_args,\n                        parser_args=parser_args,\n                        responder=self.name,\n                        extra=extra,\n                        api_type=self.api_type,\n                    )\n                    working_dialog.append(response) \n                    if response.execution_errors != []:\n                        execution_attempts.append(response)\n                        raise AgentException(response.error_message)\n                    else: \n                        break\n                except AgentException as e: # handle the exception from the agent\n                    if exception_retry &gt; 0:\n                        exception_retry -= 1\n                        U.cprint(f'{self.name} is handling an exception {e}, retry times: {self.max_exception_retry-exception_retry}/{self.max_exception_retry}','r')\n                        working_dialog.send_message(current_prompt.exception_handler, {'error_message': str(e)}, creator='exception')\n                        current_prompt = dialog.top_prompt\n                        continue\n                    else:\n                        raise e\n                except Exception as e: # handle the exception from the LLM\n                    # Simplified error handling for now\n                    wait_time = random.random()*15+1\n                    if U.is_openai_rate_limit_error(e): # for safe\n                        time.sleep(wait_time)\n                    else:\n                        if llm_recall &gt; 0:\n                            llm_recall -= 1\n                            time.sleep(1) # wait for a while before retrying\n                            continue\n                        else:\n                            raise e\n\n            response.execution_attempts = execution_attempts\n            dialog.append(response) # update the dialog state\n            # now handle the interruption\n            if response.is_function_call:\n                _func_names = [func_call.name for func_call in response.function_calls]\n                U.cprint(f'{self.name} is calling function {_func_names}, interrupt times: {i+1}/{self.max_interrupt_times}','y')\n                # handle the function call\n                for function_call in response.function_calls:\n                    if function_call.is_repeated(interrupts):\n                        result_str = f'The function {function_call.name} with identical arguments {function_call.arguments} has been called earlier, please check the previous results and do not call it again. If you do not need to call more functions, just stop calling and provide the final response.'\n                    else:\n                        print(f'{self.name} is calling function {function_call.name} with arguments {function_call.arguments}')\n                        if function_call.name not in current_prompt.functions:\n                            raise KeyError(f\"Function '{function_call.name}' not registered on prompt '{current_prompt.path}'\")\n                        function = current_prompt.functions[function_call.name]\n                        function_call = function(function_call)\n                        result_str = function_call.result_str\n                        interrupts.append(function_call)\n                    if response.api_type == APITypes.RESPONSE:\n                        interrupt_role = Roles.USER\n                    else:\n                        interrupt_role = Roles.TOOL\n                    dialog.send_message(\n                        current_prompt.interrupt_handler,\n                        {'call_results': result_str},\n                        role=interrupt_role,\n                        creator='function',\n                        extra={'tool_call_id': function_call.id},\n                    )\n                if i == self.max_interrupt_times-1:\n                    dialog.send_message(current_prompt.interrupt_handler_final, role=Roles.USER, creator='function')\n                current_prompt = dialog.top_prompt\n            else: # the response is not a function call, it is the final response\n                if i &gt; 0:   \n                    U.cprint(f'{self.name} stopped calling functions, total interrupt times: {i}/{self.max_interrupt_times}','y')\n                return response, dialog, interrupts\n        raise ValueError('Failed to call the agent')\n\n    # a special agent call for classification\n    def _classify(self, dialog: Dialog, classes: List[str], classifier_args: Dict[str, Any]):\n        _, dialog, _ = self.call(dialog, args=classifier_args)\n        response = dialog.tail.raw_response\n        choice = response.choices[0]\n        logprobs = choice.logprobs.content\n        if not len(logprobs) == 1:\n            raise ClassificationError(f'Failed to classify the proposition, not only one token ({len(logprobs)})', {})\n        chosen_token_data = choice.logprobs.content[0]\n        top_probs = {}\n        for top_logprob_entry in chosen_token_data.top_logprobs:\n            token = top_logprob_entry.token\n            prob = np.exp(top_logprob_entry.logprob)\n            top_probs[token] = prob\n        U.cprint(top_probs, 'y')\n        errors = []\n        for token in classes:\n            if token not in top_probs:\n                errors.append(f\"Token {token} not found in the top logprobs\")\n        if errors != []:\n            raise ClassificationError(f'Failed to classify the proposition:\\n{\"\\n\".join(errors)}', {})\n        return top_probs, dialog\n\n    def classify(self, dialog: Dialog, classes: List[str], classifier_prompt: str = None, strength: int = 10):\n        # binary classification by default\n        _classifier_args = self.model_card.make_classifier(classes, strength)\n        if classifier_prompt is None:\n            _classes = ' or '.join([f'\"{t}\"' for t in classes])\n            classifier_prompt = f\"Please respond with one and only one word from {_classes}.\"\n        dialog.send_message(classifier_prompt)\n        _dialog = dialog.fork()\n        llm_recall = self.max_llm_recall \n        exception_retry = self.max_exception_retry \n        while True:\n            try:\n                top_probs, _dialog = self._classify(_dialog, classes, _classifier_args)\n                dialog.append(_dialog.tail) # truncate the error handlings\n                return top_probs, dialog\n            except ClassificationError as e:\n                if exception_retry &gt; 0:\n                    _dialog.send_message(f'Please respond with one and only one word from {classes}')\n                    exception_retry -= 1\n                    print(f'{e}\\nRetrying... times: {self.max_exception_retry-exception_retry}/{self.max_exception_retry}')\n                else:\n                    raise e\n            except Exception as e:\n                if llm_recall &gt; 0:\n                    llm_recall -= 1\n                    time.sleep(1)\n                    continue\n                else:\n                    raise e\n</code></pre>"},{"location":"reference/core/#lllm.core.agent.Agent.max_llm_recall","title":"<code>max_llm_recall = 0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Represents a single LLM agent with a specific role and capabilities.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name or role of the agent (e.g., 'assistant', 'coder').</p> <code>system_prompt</code> <code>Prompt</code> <p>The system prompt defining the agent's persona.</p> <code>model</code> <code>str</code> <p>The model identifier (e.g., 'gpt-4o').</p> <code>llm_provider</code> <code>BaseProvider</code> <p>The provider instance for LLM calls.</p> <code>log_base</code> <code>ReplayableLogBase</code> <p>Logger for recording interactions.</p> <code>model_args</code> <code>Dict[str, Any]</code> <p>Additional model arguments (temp, top_p, etc.).</p> <code>max_exception_retry</code> <code>int</code> <p>Max retries for agent exceptions.</p> <code>max_interrupt_times</code> <code>int</code> <p>Max consecutive tool call interrupts.</p> <code>max_llm_recall</code> <code>int</code> <p>Max retries for LLM API errors.</p>"},{"location":"reference/core/#lllm.core.agent.Agent.call","title":"<code>call(dialog, extra=None, args=None, parser_args=None)</code>","text":"<p>Executes the agent loop, handling LLM calls, tool execution, and interrupts.</p> <p>Parameters:</p> Name Type Description Default <code>dialog</code> <code>Dialog</code> <p>The current dialog state.</p> required <code>extra</code> <code>Dict[str, Any]</code> <p>Extra metadata for the call.</p> <code>None</code> <code>args</code> <code>Dict[str, Any]</code> <p>Additional arguments for the prompt.</p> <code>None</code> <code>parser_args</code> <code>Dict[str, Any]</code> <p>Arguments for the output parser.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[Message, Dialog, List[FunctionCall]]</code> <p>Tuple[Message, Dialog, List[FunctionCall]]: The final response message, the updated dialog, and a list of executed function calls.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the agent fails to produce a valid response after retries.</p> Source code in <code>lllm/core/agent.py</code> <pre><code>def call(\n    self,\n    dialog: Dialog,  # it assumes the prompt is already loaded into the dialog as the top prompt by send_message\n    extra: Optional[Dict[str, Any]] = None,  # for tracking additional information, such as frontend replay info\n    args: Optional[Dict[str, Any]] = None,  # for tracking additional information, such as frontend replay info\n    parser_args: Optional[Dict[str, Any]] = None,\n) -&gt; Tuple[Message, Dialog, List[FunctionCall]]:\n    \"\"\"\n    Executes the agent loop, handling LLM calls, tool execution, and interrupts.\n\n    Args:\n        dialog (Dialog): The current dialog state.\n        extra (Dict[str, Any], optional): Extra metadata for the call.\n        args (Dict[str, Any], optional): Additional arguments for the prompt.\n        parser_args (Dict[str, Any], optional): Arguments for the output parser.\n\n    Returns:\n        Tuple[Message, Dialog, List[FunctionCall]]: The final response message, the updated dialog, and a list of executed function calls.\n\n    Raises:\n        ValueError: If the agent fails to produce a valid response after retries.\n    \"\"\"\n    extra = dict(extra) if extra else {}\n    args = dict(args) if args else {}\n    parser_args = dict(parser_args) if parser_args else {}\n    # Prompt: a function maps prompt args and dialog into the expected output \n    current_prompt = dialog.top_prompt or self.system_prompt\n    interrupts = []\n    for i in range(10000 if self.max_interrupt_times == 0 else self.max_interrupt_times+1): # +1 for the final response\n        llm_recall = self.max_llm_recall \n        exception_retry = self.max_exception_retry \n        working_dialog = dialog.fork() # make a copy of the dialog, truncate all excception handling dialogs\n        while True: # ensure the response is no exception\n            execution_attempts = []\n            try:\n                _model_args = self.model_args.copy()\n                _model_args.update(args)\n\n                response = self.llm_provider.call(\n                    working_dialog,\n                    current_prompt,\n                    self.model,\n                    _model_args,\n                    parser_args=parser_args,\n                    responder=self.name,\n                    extra=extra,\n                    api_type=self.api_type,\n                )\n                working_dialog.append(response) \n                if response.execution_errors != []:\n                    execution_attempts.append(response)\n                    raise AgentException(response.error_message)\n                else: \n                    break\n            except AgentException as e: # handle the exception from the agent\n                if exception_retry &gt; 0:\n                    exception_retry -= 1\n                    U.cprint(f'{self.name} is handling an exception {e}, retry times: {self.max_exception_retry-exception_retry}/{self.max_exception_retry}','r')\n                    working_dialog.send_message(current_prompt.exception_handler, {'error_message': str(e)}, creator='exception')\n                    current_prompt = dialog.top_prompt\n                    continue\n                else:\n                    raise e\n            except Exception as e: # handle the exception from the LLM\n                # Simplified error handling for now\n                wait_time = random.random()*15+1\n                if U.is_openai_rate_limit_error(e): # for safe\n                    time.sleep(wait_time)\n                else:\n                    if llm_recall &gt; 0:\n                        llm_recall -= 1\n                        time.sleep(1) # wait for a while before retrying\n                        continue\n                    else:\n                        raise e\n\n        response.execution_attempts = execution_attempts\n        dialog.append(response) # update the dialog state\n        # now handle the interruption\n        if response.is_function_call:\n            _func_names = [func_call.name for func_call in response.function_calls]\n            U.cprint(f'{self.name} is calling function {_func_names}, interrupt times: {i+1}/{self.max_interrupt_times}','y')\n            # handle the function call\n            for function_call in response.function_calls:\n                if function_call.is_repeated(interrupts):\n                    result_str = f'The function {function_call.name} with identical arguments {function_call.arguments} has been called earlier, please check the previous results and do not call it again. If you do not need to call more functions, just stop calling and provide the final response.'\n                else:\n                    print(f'{self.name} is calling function {function_call.name} with arguments {function_call.arguments}')\n                    if function_call.name not in current_prompt.functions:\n                        raise KeyError(f\"Function '{function_call.name}' not registered on prompt '{current_prompt.path}'\")\n                    function = current_prompt.functions[function_call.name]\n                    function_call = function(function_call)\n                    result_str = function_call.result_str\n                    interrupts.append(function_call)\n                if response.api_type == APITypes.RESPONSE:\n                    interrupt_role = Roles.USER\n                else:\n                    interrupt_role = Roles.TOOL\n                dialog.send_message(\n                    current_prompt.interrupt_handler,\n                    {'call_results': result_str},\n                    role=interrupt_role,\n                    creator='function',\n                    extra={'tool_call_id': function_call.id},\n                )\n            if i == self.max_interrupt_times-1:\n                dialog.send_message(current_prompt.interrupt_handler_final, role=Roles.USER, creator='function')\n            current_prompt = dialog.top_prompt\n        else: # the response is not a function call, it is the final response\n            if i &gt; 0:   \n                U.cprint(f'{self.name} stopped calling functions, total interrupt times: {i}/{self.max_interrupt_times}','y')\n            return response, dialog, interrupts\n    raise ValueError('Failed to call the agent')\n</code></pre>"},{"location":"reference/core/#agentbase","title":"AgentBase","text":""},{"location":"reference/core/#lllm.core.agent.AgentBase","title":"<code>lllm.core.agent.AgentBase</code>","text":"Source code in <code>lllm/core/agent.py</code> <pre><code>class AgentBase:\n    agent_type: str | Enum = None\n    agent_group: List[str] = None\n    is_async: bool = False\n\n    def __init_subclass__(cls, register: bool = True, **kwargs):\n        super().__init_subclass__(**kwargs)\n        if register:\n            register_agent_class(cls)\n\n    def __init__(self, config: Dict[str, Any], ckpt_dir: str, stream = None):\n        auto_discover_if_enabled(config.get(\"auto_discover\"))\n        if stream is None:\n            stream = U.PrintSystem()\n        self.config = config\n        assert self.agent_group is not None, f\"Agent group is not set for {self.agent_type}\"\n        _agent_configs = config['agent_configs']\n        self.agent_configs = {}\n        for agent_name in self.agent_group:\n            assert agent_name in _agent_configs, f\"Agent {agent_name} not found in agent configs\"\n            self.agent_configs[agent_name] = _agent_configs[agent_name]\n        self._stream = stream\n        self._stream_backup = stream\n        self.st = None\n        self.ckpt_dir = ckpt_dir\n        self._log_base = build_log_base(config)\n        self.agents = {}\n\n        # Initialize Provider via registry\n        self.llm_provider = build_provider(config)\n\n        for agent_name, model_config in self.agent_configs.items():\n            model_config = model_config.copy()\n            model_name = model_config.pop('model_name')\n            self.model = find_model_card(model_name)\n            system_prompt_path = model_config.pop('system_prompt_path')\n            api_type_value = model_config.pop('api_type', APITypes.COMPLETION.value)\n            if isinstance(api_type_value, APITypes):\n                api_type = api_type_value\n            else:\n                api_type = APITypes(api_type_value)\n\n            # We assume PROMPT_REGISTRY is available globally or passed. \n            # This is a bit of a dependency issue. \n            # Ideally, prompts should be loaded/registered before Agent initialization.\n            # For now, we'll assume the user registers prompts before creating agents.\n            from lllm.core.models import PROMPT_REGISTRY\n\n            self.agents[agent_name] = Agent(\n                name=agent_name,\n                system_prompt=PROMPT_REGISTRY[system_prompt_path],\n                model=model_name,\n                llm_provider=self.llm_provider,\n                api_type=api_type,\n                model_args=model_config,\n                log_base=self._log_base,\n                max_exception_retry=self.config.get('max_exception_retry', 3),\n                max_interrupt_times=self.config.get('max_interrupt_times', 5),\n                max_llm_recall=self.config.get('max_llm_recall', 0),\n            )\n\n        self.__additional_args = {}\n        sig = inspect.signature(self.call)\n        for arg in sig.parameters:\n            if arg not in {'task', '**kwargs'}:\n                self.__additional_args[arg] = sig.parameters[arg].default\n\n    def set_st(self, session_name: str):\n        self.st = U.StreamWrapper(self._stream, self._log_base, session_name)\n\n    def restore_st(self):\n        pass\n\n    def silent(self):\n        self._stream = U.PrintSystem(silent=True)\n\n    def restore(self):\n        self._stream = self._stream_backup\n\n    def call(self, task: str, **kwargs):\n        raise NotImplementedError\n\n    def __call__(self, task: str, session_name: str = None, **kwargs) -&gt; str:\n        if session_name is None:\n            session_name = task.replace(' ', '+')+'_'+dt.datetime.now().strftime('%Y%m%d_%H%M%S')\n        self.set_st(session_name)\n        report = self.call(task, **kwargs)\n        with self.st.expander('Prediction Overview', expanded=True):\n            self.st.code(f'{report}')\n        self.restore_st()\n        return report\n</code></pre>"},{"location":"reference/core/#dialog","title":"Dialog","text":""},{"location":"reference/core/#lllm.core.dialog.Dialog","title":"<code>lllm.core.dialog.Dialog</code>  <code>dataclass</code>","text":"<p>Whenever a dialog is created/forked, it should be associated with a session name</p> Source code in <code>lllm/core/dialog.py</code> <pre><code>@dataclass\nclass Dialog:\n    \"\"\"\n    Whenever a dialog is created/forked, it should be associated with a session name\n    \"\"\"\n    _messages: List[Message]\n    log_base: ReplayableLogBase\n    session_name: str\n    parent_dialog: Optional[str] = None\n    top_prompt: Optional[Prompt] = None\n\n    def __post_init__(self):\n        self.dialog_id = uuid.uuid4().hex\n        dialogs_sess = self.log_base.get_collection(RCollections.DIALOGS).create_session(self.session_name) # track the dialogs created in this session\n        dialogs_sess.log(self.dialog_id, metadata={'parent_dialog': self.parent_dialog})\n        self.sess = self.log_base.get_collection(RCollections.MESSAGES).create_session(f'{self.session_name}/{self.dialog_id}') # track the dialogs created in this session\n\n    def append(self, message: Message): # ensure this is the only way to write the messages to make sure the trackability\n        message.extra['dialog_id'] = self.dialog_id\n        self._messages.append(message)\n        try:\n            self.sess.log(message.content, metadata=message.to_dict()) # Use to_dict for logging\n        except Exception as e:\n            print(f'WARNING: Failed to log message: {e}, log the message without metadata')\n            self.sess.log(message.content)\n\n\n    def to_dict(self):\n        return {\n            'messages': [message.to_dict() for message in self._messages],\n            'session_name': self.session_name,\n            'parent_dialog': self.parent_dialog,\n            'top_prompt_path': self.top_prompt.path if self.top_prompt is not None else None,\n        }\n\n    @classmethod\n    def from_dict(cls, d: dict, log_base: ReplayableLogBase, prompt_registry: Dict[str, Prompt]):\n        top_prompt_path = d['top_prompt_path']\n        if top_prompt_path is not None:\n            # Assuming PROMPT_REGISTRY is available or passed. \n            # For now, we rely on the passed prompt_registry.\n            top_prompt = prompt_registry.get(top_prompt_path)\n            if top_prompt is None:\n                 print(f\"Warning: Prompt {top_prompt_path} not found in registry.\")\n        else:\n            top_prompt = None\n        return cls(\n            _messages=[Message.from_dict(message) for message in d['messages']],\n            log_base=log_base,\n            session_name=d['session_name'],\n            parent_dialog=d['parent_dialog'],\n            top_prompt=top_prompt,\n        )\n\n    @property\n    def messages(self):\n        return self._messages\n\n    def send_base64_image(\n        self,\n        image_base64: str,\n        caption: str = None,\n        creator: str = 'user',\n        extra: Optional[Dict[str, Any]] = None,\n        role: Roles = Roles.USER,\n    ) -&gt; Message:\n        payload = dict(extra) if extra else {}\n        if caption is not None:\n            payload['caption'] = caption\n        message = Message(\n            role=role,\n            content=image_base64,\n            creator=creator,\n            modality=Modalities.IMAGE,\n            extra=payload,\n        )\n        self.append(message)\n        return message\n\n    def send_message(\n        self,\n        prompt: Prompt | str,\n        prompt_args: Optional[Dict[str, Any]] = None,\n        creator: str = 'user',  # or 'user', etc.\n        extra: Optional[Dict[str, Any]] = None,\n        role: Roles = Roles.USER,\n    ) -&gt; Message:\n        prompt_args = dict(prompt_args) if prompt_args else {}\n        metadata = dict(extra) if extra else {}\n        if isinstance(prompt, str):\n            assert not prompt_args, \"Prompt args are not allowed for string prompt\"\n            # Create a temporary prompt object\n            prompt = Prompt(path='__temp_prompt_'+str(uuid.uuid4())[:6], prompt=prompt)\n            content = prompt.prompt\n        elif not prompt_args:\n            content = prompt.prompt\n        else:\n            content = prompt(**prompt_args)\n        message = Message(\n            role=role,\n            content=content,\n            creator=creator,\n            modality=Modalities.TEXT,\n            extra=metadata\n        )\n        self.append(message)\n        self.top_prompt = prompt\n        return message\n\n    def fork(self) -&gt; 'Dialog':\n        _messages = [copy.deepcopy(message) for message in self._messages]\n        _dialog = Dialog(_messages, self.log_base, self.session_name, self.dialog_id)\n        _dialog.top_prompt = self.top_prompt\n        return _dialog\n\n    def overview(self, remove_tail: bool = False, max_length: int = 100, \n                 stream = None, divider: bool = False):\n        _overview = ''\n        for idx, message in enumerate(self.messages):\n            if remove_tail and idx == len(self.messages)-1:\n                break\n            # message.overview() logic needs to be in Message class or here\n            # Message class has overview method in original code, I should add it back to Message model if I missed it\n            # I missed it in Message model. I'll implement a simple one here or add it to Message.\n            # Let's assume Message has it or I implement it here.\n            # Implementing here for safety if I missed it in Pydantic model.\n            content_preview = str(message.content)[:max_length] + '...' if len(str(message.content)) &gt; max_length else str(message.content)\n            _overview += f'[{idx}. {message.creator} ({message.role.value})]: {content_preview}\\n\\n'\n\n        _overview = _overview.strip()\n        cost = self.tail.cost if self.messages else CompletionCost()\n        if stream is not None:\n            if divider:\n                stream.divider()\n            stream.write(U.html_collapse(f'Context overview', _overview), unsafe_allow_html=True)\n            stream.write(str(cost))\n        return _overview\n\n    @property\n    def tail(self): # last message in the dialog, use it to get last response from the LLM\n        return self._messages[-1] if self._messages else None\n\n    @property\n    def system(self):\n        return self._messages[0] if self._messages else None\n\n    def context_copy(self, n: int = 1) -&gt; 'Dialog':\n        _dialog = self.fork()\n        if n &gt; 0:\n            _dialog._messages = _dialog._messages[:-n]\n        return _dialog\n\n    @property\n    def cost(self) -&gt; CompletionCost:\n        return self.get_cost()\n\n    def get_cost(self, model: str = None) -&gt; CompletionCost:\n        costs = [message.cost for message in self._messages]\n        return CompletionCost(\n            prompt_tokens=sum([cost.prompt_tokens for cost in costs]),\n            completion_tokens=sum([cost.completion_tokens for cost in costs]),\n            cached_prompt_tokens=sum([cost.cached_prompt_tokens for cost in costs]),\n            cost=sum([cost.cost for cost in costs])\n        )\n</code></pre>"},{"location":"reference/core/#models","title":"Models","text":""},{"location":"reference/core/#lllm.core.models.Message","title":"<code>lllm.core.models.Message</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>lllm/core/models.py</code> <pre><code>class Message(BaseModel):\n    role: Roles\n    content: Union[str, List[Dict[str, Any]]] # Content can be string or list of content parts (for images)\n    creator: str\n    raw_response: Any = None\n    function_calls: List[FunctionCall] = Field(default_factory=list)\n    modality: Modalities = Modalities.TEXT\n    logprobs: List[TokenLogprob] = Field(default_factory=list)\n    parsed: Dict[str, Any] = Field(default_factory=dict)\n    model: Optional[str] = None\n    usage: Dict[str, float] = Field(default_factory=dict)\n    model_args: Dict[str, Any] = Field(default_factory=dict)\n    extra: Dict[str, Any] = Field(default_factory=dict)\n    execution_errors: List[Exception] = Field(default_factory=list)\n    execution_attempts: List['Message'] = Field(default_factory=list)\n    api_type: APITypes = APITypes.COMPLETION\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @field_validator(\"logprobs\", mode=\"before\")\n    @classmethod\n    def _coerce_logprobs(cls, value):\n        if not value:\n            return []\n        normalized: List[TokenLogprob] = []\n        for entry in value:\n            if isinstance(entry, TokenLogprob):\n                normalized.append(entry)\n                continue\n            if isinstance(entry, dict):\n                normalized.append(TokenLogprob(**entry))\n                continue\n            if isinstance(entry, (int, float)):\n                normalized.append(TokenLogprob(logprob=float(entry)))\n                continue\n            normalized.append(TokenLogprob(token=str(entry)))\n        return normalized\n\n    @property\n    def error_message(self):\n        return '\\n'.join([str(e) for e in self.execution_errors])\n\n    @property\n    def cost(self) -&gt; CompletionCost:\n        if self.model is None or not self.usage:\n            return CompletionCost()\n        try:\n            card = find_model_card(self.model)\n        except Exception:\n            return CompletionCost()\n        return card.cost(self.usage)\n\n    @property\n    def is_function_call(self) -&gt; bool:\n        return len(self.function_calls) &gt; 0\n\n    def to_dict(self):\n        return self.model_dump(exclude={'raw_response', 'execution_errors', 'execution_attempts'})\n\n    @classmethod\n    def from_dict(cls, d: dict):\n        return cls(**d)\n</code></pre>"},{"location":"reference/core/#lllm.core.models.Prompt","title":"<code>lllm.core.models.Prompt</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>lllm/core/models.py</code> <pre><code>class Prompt(BaseModel):\n    path: str\n    prompt: str\n    functions_list: List[Function] = Field(default_factory=list)\n    mcp_servers_list: List[MCP] = Field(default_factory=list)\n    parser: Optional[Callable[[str], Dict[str, Any]]] = None\n    exception_prompt: str = \"Error: {error_message}. Please fix.\"\n    interrupt_prompt: str = \"Result: {call_results}. Continue?\"\n    interrupt_final_prompt: str = \"All tool calls are done. Provide the final response.\"\n    format: Optional[Any] = None # Pydantic model class for structured output\n    xml_tags: List[str] = Field(default_factory=list)\n    md_tags: List[str] = Field(default_factory=list)\n    signal_tags: List[str] = Field(default_factory=list)\n    required_xml_tags: List[str] = Field(default_factory=list)\n    required_md_tags: List[str] = Field(default_factory=list)\n    allow_web_search: bool = False\n    computer_use_config: Dict[str, Any] = Field(default_factory=dict)\n\n    functions: Dict[str, Function] = Field(default_factory=dict, init=False)\n    mcp_servers: Dict[str, MCP] = Field(default_factory=dict, init=False)\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    def model_post_init(self, __context):\n        self.functions = {f.name: f for f in self.functions_list}\n        self.mcp_servers = {m.server_label: m for m in self.mcp_servers_list}\n\n    def link_function(self, name: str, function: Callable):\n        if name in self.functions:\n            self.functions[name].link_function(function)\n\n    def register_mcp_server(self, server: MCP):\n        self.mcp_servers[server.server_label] = server\n\n    def __call__(self, **kwargs):\n        if not kwargs:\n            return self.prompt\n        return self.prompt.format(**kwargs)\n\n    @property\n    def exception_handler(self):\n        # Recursive prompt creation logic (simplified for now)\n        return Prompt(\n            path=f'__{self.path}_exception_handler',\n            prompt=self.exception_prompt,\n            parser=self.parser,\n            functions_list=self.functions_list,\n            mcp_servers_list=self.mcp_servers_list,\n            exception_prompt=self.exception_prompt,\n            interrupt_prompt=self.interrupt_prompt,\n            format=self.format,\n            xml_tags=self.xml_tags,\n            md_tags=self.md_tags,\n            signal_tags=self.signal_tags,\n            required_xml_tags=self.required_xml_tags,\n            required_md_tags=self.required_md_tags,\n            allow_web_search=self.allow_web_search,\n            computer_use_config=self.computer_use_config,\n            interrupt_final_prompt=self.interrupt_final_prompt,\n        )\n\n    @property\n    def interrupt_handler(self):\n         return Prompt(\n            path=f'__{self.path}_interrupt_handler',\n            prompt=self.interrupt_prompt,\n            parser=self.parser,\n            functions_list=self.functions_list,\n            mcp_servers_list=self.mcp_servers_list,\n            exception_prompt=self.exception_prompt,\n            interrupt_prompt=self.interrupt_prompt,\n            format=self.format,\n            xml_tags=self.xml_tags,\n            md_tags=self.md_tags,\n            signal_tags=self.signal_tags,\n            required_xml_tags=self.required_xml_tags,\n            required_md_tags=self.required_md_tags,\n            allow_web_search=self.allow_web_search,\n            computer_use_config=self.computer_use_config,\n            interrupt_final_prompt=self.interrupt_final_prompt,\n        )\n\n    @property\n    def interrupt_handler_final(self):\n         return Prompt(\n            path=f'__{self.path}_interrupt_handler_final',\n            prompt=self.interrupt_final_prompt,\n            parser=self.parser,\n            functions_list=self.functions_list,\n            mcp_servers_list=self.mcp_servers_list,\n            exception_prompt=self.exception_prompt,\n            interrupt_prompt=self.interrupt_prompt,\n            interrupt_final_prompt=self.interrupt_final_prompt,\n            format=self.format,\n            xml_tags=self.xml_tags,\n            md_tags=self.md_tags,\n            signal_tags=self.signal_tags,\n            required_xml_tags=self.required_xml_tags,\n            required_md_tags=self.required_md_tags,\n            allow_web_search=self.allow_web_search,\n            computer_use_config=self.computer_use_config,\n        )\n</code></pre>"},{"location":"reference/modules/","title":"Module Reference","text":"<p>This section lists the primary modules in the repository with short descriptions so you can jump into the right file quickly.</p>"},{"location":"reference/modules/#core-package-lllm","title":"Core Package (<code>lllm/</code>)","text":"File Description <code>lllm/__init__.py</code> Public package exports and <code>auto_discover()</code> invocation. <code>lllm/llm.py</code> Agent framework (dialogs, agent call loop, classifier helpers, prompt registry utilities). <code>lllm/models.py</code> Dataclasses for <code>Prompt</code>, <code>Message</code>, <code>Function</code>, <code>FunctionCall</code>, MCP descriptors, and parsing helpers. <code>lllm/const.py</code> Enumerations (roles, providers, features), model cards, pricing utilities, and tokenizer helpers. <code>lllm/providers/</code> Provider registry plus concrete backends (currently OpenAI chat/responses) implementing <code>BaseProvider</code>. <code>lllm/log.py</code> Replayable logging base classes plus <code>LocalFileLog</code> and <code>NoLog</code>. <code>lllm/utils.py</code> Filesystem helpers, caching, HTML utilities, frontend wrappers, and HTTP helper functions (<code>call_api</code>, <code>directory_tree</code>, etc.). <code>lllm/config.py</code> Functions for locating and loading <code>lllm.toml</code>, plus environment-variable guards. <code>lllm/discovery.py</code> Auto-discovery of prompts and proxies based on <code>lllm.toml</code>; includes module loader helpers. <code>lllm/proxies.py</code> <code>BaseProxy</code>, endpoint decorators, runtime <code>Proxy</code>, and supporting utilities (auto docs, auto tests, cutoff filtering). <code>lllm/sandbox.py</code> Notebook sandbox with <code>JupyterSession</code>, kernel management, and helper enums for programmatic execution. <code>lllm/tools/cua.py</code> Experimental Computer Use Agent for Playwright-driven browser automation. <code>lllm/cli.py</code> Implementation of the <code>lllm</code> command-line interface and template renderer. <code>lllm/README.md</code> Additional background on agent calls, prompts, and handlers."},{"location":"reference/modules/#templates-examples-template","title":"Templates &amp; Examples (<code>template/</code>)","text":"Path Description <code>template/init_template/</code> Minimal scaffold used by <code>lllm create</code>. Includes <code>lllm.toml</code>, config stub, and <code>SimpleSystem</code>. <code>template/example/</code> Comprehensive example with system orchestration, prompt definitions, proxy modules, and MCP wiring. <code>template/example/system/system.py</code> Demonstrates experiment/stream management, sync/async call wrappers, and error handling. <code>template/example/system/agent/agent.py</code> Implements the <code>Vanilla</code> agent that orchestrates prompts and dialogs. <code>template/example/system/proxy/modules/*.py</code> Realistic proxy implementations (financial data, Google Trends, FRED, Wolfram Alpha, etc.). <p>Use this table as a quick index when contributing or debugging.</p>"},{"location":"reference/providers/","title":"Providers API Reference","text":""},{"location":"reference/providers/#baseprovider","title":"BaseProvider","text":""},{"location":"reference/providers/#lllm.providers.base.BaseProvider","title":"<code>lllm.providers.base.BaseProvider</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>lllm/providers/base.py</code> <pre><code>class BaseProvider(ABC):\n    @abstractmethod\n    def call(\n        self,\n        dialog: Any,  # Typed as Any to avoid circular import with Dialog, or use 'Dialog' forward ref\n        prompt: Prompt,\n        model: str,\n        model_args: Optional[Dict[str, Any]] = None,\n        parser_args: Optional[Dict[str, Any]] = None,\n        responder: str = 'assistant',\n        extra: Optional[Dict[str, Any]] = None,\n        api_type: APITypes = APITypes.COMPLETION,\n    ) -&gt; Message:\n        pass\n\n    @abstractmethod\n    def stream(\n        self,\n        dialog: Any,\n        prompt: Prompt,\n        model: str,\n        model_args: Optional[Dict[str, Any]] = None,\n        parser_args: Optional[Dict[str, Any]] = None,\n        responder: str = 'assistant',\n        extra: Optional[Dict[str, Any]] = None,\n        api_type: APITypes = APITypes.COMPLETION,\n    ) -&gt; Generator[Any, None, Message]:\n        pass\n</code></pre>"},{"location":"reference/providers/#openaiprovider","title":"OpenAIProvider","text":""},{"location":"reference/providers/#lllm.providers.openai.OpenAIProvider","title":"<code>lllm.providers.openai.OpenAIProvider</code>","text":"<p>               Bases: <code>BaseProvider</code></p> Source code in <code>lllm/providers/openai.py</code> <pre><code>class OpenAIProvider(BaseProvider):\n    def __init__(self, config: Optional[Dict[str, Any]] = None):\n        config = config or {}\n        self._api_key = config.get(\"api_key\") or os.getenv(\"OPENAI_API_KEY\")\n        if not self._api_key:\n            raise RuntimeError(\"OPENAI_API_KEY is not set\")\n        self.client = openai.OpenAI(api_key=self._api_key) # Preserving env var name\n        # Support for other base_urls (e.g. Together AI)\n        together_api_key = config.get(\"together_api_key\") or os.getenv('TOGETHER_API_KEY')\n        if together_api_key is not None:\n            self.together_client = openai.OpenAI(api_key=together_api_key, base_url='https://api.together.xyz/v1')\n        else:\n            self.together_client = None\n            print(\"TOGETHER_API_KEY is not set, cannot use Together AI models\")\n\n    def _get_client(self, model: str):\n        model_card = find_model_card(model)\n        if model_card.base_url is not None:\n            if 'together' in model_card.base_url:\n                return self.together_client\n            else:\n                # Generic base_url support could be added here\n                return openai.OpenAI(api_key=self._api_key, base_url=model_card.base_url)\n        return self.client\n\n    def _convert_dialog(self, dialog: Any) -&gt; List[Dict[str, Any]]:\n        \"\"\"Convert internal Dialog state into OpenAI-compatible messages.\"\"\"\n        messages: List[Dict[str, Any]] = []\n        for message in dialog.messages:\n            if message.role in (Roles.ASSISTANT, Roles.TOOL_CALL):\n                assistant_entry: Dict[str, Any] = {\n                    \"role\": \"assistant\",\n                    \"content\": message.content,\n                }\n                if message.function_calls:\n                    assistant_entry[\"tool_calls\"] = [\n                        {\n                            \"id\": fc.id,\n                            \"type\": \"function\",\n                            \"function\": {\n                                \"name\": fc.name,\n                                \"arguments\": json.dumps(fc.arguments),\n                            },\n                        }\n                        for fc in message.function_calls\n                    ]\n                messages.append(assistant_entry)\n                continue\n\n            if message.role == Roles.TOOL:\n                tool_call_id = message.extra.get(\"tool_call_id\")\n                if not tool_call_id:\n                    raise ValueError(\n                        \"Tool call id is not found in the message extra for tool message: \"\n                        f\"{message}\"\n                    )\n                messages.append(\n                    {\n                        \"role\": \"tool\",\n                        \"content\": message.content,\n                        \"tool_call_id\": tool_call_id,\n                    }\n                )\n                continue\n\n            if message.modality == Modalities.IMAGE:\n                content_parts = []\n                if \"caption\" in message.extra:\n                    content_parts.append({\"type\": \"text\", \"text\": message.extra[\"caption\"]})\n                content_parts.append(\n                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{message.content}\"}}\n                )\n                messages.append({\"role\": message.role.openai, \"content\": content_parts})\n                continue\n\n            if message.modality == Modalities.TEXT:\n                messages.append({\"role\": message.role.openai, \"content\": message.content})\n                continue\n\n            raise ValueError(f\"Unsupported modality: {message.modality}\")\n\n        return messages\n\n    def _call_chat_api(\n        self,\n        dialog: Any,\n        prompt: Prompt,\n        model: str,\n        model_card,\n        client,\n        payload_args: Dict[str, Any],\n        parser_args: Dict[str, Any],\n        responder: str,\n        extra: Dict[str, Any],\n    ) -&gt; Message:\n        tools = self._build_tools(prompt)\n        call_args = dict(payload_args)\n\n        if prompt.format is None:\n            call_fn = client.chat.completions.create\n        else:\n            call_fn = client.beta.chat.completions.parse\n            call_args['response_format'] = prompt.format\n\n        if model_card.is_reasoning:\n            call_args['temperature'] = call_args.get('temperature', 1)\n\n        completion = call_fn(\n            model=model,\n            messages=self._convert_dialog(dialog),\n            tools=tools if tools else None,\n            **call_args,\n        )\n\n        choice = completion.choices[0]\n        usage = json.loads(completion.usage.model_dump_json())\n\n        if choice.finish_reason == 'tool_calls':\n            role = Roles.TOOL_CALL\n            logprobs = None\n            parsed = None\n            errors: List[Exception] = []\n            function_calls = [\n                FunctionCall(\n                    id=tool_call.id,\n                    name=tool_call.function.name,\n                    arguments=json.loads(tool_call.function.arguments),\n                )\n                for tool_call in choice.message.tool_calls\n            ]\n            content = 'Tool calls:\\n\\n' + '\\n'.join(\n                [\n                    f'{idx}. {tool_call.function.name}: {tool_call.function.arguments}'\n                    for idx, tool_call in enumerate(choice.message.tool_calls)\n                ]\n            )\n        else:\n            role = Roles.ASSISTANT\n            errors = []\n            function_calls = []\n\n            if prompt.format is None:\n                content = choice.message.content\n                raw_logprobs = choice.logprobs.content if choice.logprobs is not None else None\n                if raw_logprobs is not None:\n                    converted = []\n                    for logprob in raw_logprobs:\n                        payload = logprob.model_dump() if hasattr(logprob, \"model_dump\") else logprob\n                        converted.append(TokenLogprob.model_validate(payload))\n                    logprobs = converted\n                else:\n                    logprobs = None\n                try:\n                    parsed = prompt.parser(content, **parser_args) if prompt.parser is not None else None\n                except Exception as exc:\n                    errors.append(exc)\n                    parsed = {'raw': content}\n            else:\n                if choice.message.refusal:\n                    raise ValueError(choice.message.refusal)\n                content = str(choice.message.parsed.json())\n                parsed = json.loads(content)\n                logprobs = None\n\n            if 'response_format' in call_args and prompt.format is not None:\n                call_args['response_format'] = prompt.format.model_json_schema()\n\n        return Message(\n            role=role,\n            raw_response=completion,\n            creator=responder,\n            function_calls=function_calls,\n            content=content,\n            logprobs=logprobs or [],\n            model=model,\n            model_args=call_args,\n            usage=usage,\n            parsed=parsed or {},\n            extra=extra,\n            execution_errors=errors,\n            api_type=APITypes.COMPLETION,\n        )\n\n    def _call_response_api(\n        self,\n        dialog: Any,\n        prompt: Prompt,\n        model: str,\n        model_card,\n        client,\n        payload_args: Dict[str, Any],\n        parser_args: Dict[str, Any],\n        responder: str,\n        extra: Dict[str, Any],\n    ) -&gt; Message:\n        if prompt.format is not None:\n            raise ValueError(\"Response API does not support structured output. Remove 'format' or use the completion API.\")\n\n        tools = self._build_tools(prompt)\n        if prompt.allow_web_search and Features.WEB_SEARCH in model_card.features:\n            tools.append({\"type\": \"web_search_preview\"})\n        if prompt.computer_use_config and Features.COMPUTER_USE in model_card.features:\n            cfg = prompt.computer_use_config\n            tools.append(\n                {\n                    \"type\": \"computer_use_preview\",\n                    \"display_width\": cfg.get(\"display_width\", 1280),\n                    \"display_height\": cfg.get(\"display_height\", 800),\n                    \"environment\": cfg.get(\"environment\", \"browser\"),\n                }\n            )\n\n        call_args = dict(payload_args)\n        max_output_tokens = call_args.pop('max_output_tokens', call_args.pop('max_completion_tokens', 32000))\n        truncation = call_args.pop('truncation', 'auto')\n        tool_choice = call_args.pop('tool_choice', 'auto')\n\n        response = client.responses.create(\n            model=model,\n            input=self._convert_dialog(dialog),\n            tools=tools if tools else None,\n            tool_choice=tool_choice,\n            max_output_tokens=max_output_tokens,\n            truncation=truncation,\n            **call_args,\n        )\n\n        usage = json.loads(response.usage.model_dump_json())\n        outputs = getattr(response, \"output\", []) or []\n        function_calls: List[FunctionCall] = []\n\n        for item in outputs:\n            if getattr(item, \"type\", None) == \"function_call\":\n                arguments = getattr(item, \"arguments\", \"{}\")\n                try:\n                    parsed_args = json.loads(arguments)\n                except Exception:\n                    parsed_args = {}\n                function_calls.append(\n                    FunctionCall(\n                        id=getattr(item, \"call_id\", getattr(item, \"id\", \"tool_call\")),\n                        name=getattr(item, \"name\", \"function\"),\n                        arguments=parsed_args,\n                    )\n                )\n\n        logprobs = None\n        errors: List[Exception] = []\n        if function_calls:\n            role = Roles.TOOL_CALL\n            parsed = None\n            content = 'Tool calls:\\n\\n' + '\\n'.join(\n                [f'{idx}. {call.name}: {json.dumps(call.arguments)}' for idx, call in enumerate(function_calls)]\n            )\n        else:\n            role = Roles.ASSISTANT\n            content = getattr(response, \"output_text\", None)\n            if not content:\n                text_chunks = []\n                for item in outputs:\n                    if getattr(item, \"type\", None) == \"output_text\":\n                        chunk = getattr(item, \"text\", None)\n                        if chunk:\n                            text_chunks.append(chunk)\n                content = '\\n'.join(text_chunks).strip()\n            try:\n                parsed = prompt.parser(content, **parser_args) if prompt.parser is not None else None\n            except Exception as exc:\n                errors.append(exc)\n                parsed = {'raw': content}\n\n        extra_payload = dict(extra)\n        reasoning = getattr(response, \"reasoning\", None)\n        if reasoning is not None:\n            try:\n                extra_payload['reasoning'] = reasoning.model_dump_json()\n            except Exception:\n                extra_payload['reasoning'] = str(reasoning)\n        extra_payload.setdefault('api_type', APITypes.RESPONSE.value)\n\n        message = Message(\n            role=role,\n            raw_response=response,\n            creator=responder,\n            function_calls=function_calls,\n            content=content,\n            logprobs=logprobs or [],\n            model=model,\n            model_args=call_args,\n            usage=usage,\n            parsed=parsed or {},\n            extra=extra_payload,\n            execution_errors=errors,\n            api_type=APITypes.RESPONSE,\n        )\n        return message\n\n    def call(\n        self,\n        dialog: Any,\n        prompt: Prompt,\n        model: str,\n        model_args: Optional[Dict[str, Any]] = None,\n        parser_args: Optional[Dict[str, Any]] = None,\n        responder: str = 'assistant',\n        extra: Optional[Dict[str, Any]] = None,\n        api_type: APITypes = APITypes.COMPLETION,\n    ) -&gt; Message:\n        model_card = find_model_card(model)\n        client = self._get_client(model)\n        payload_args = dict(model_args) if model_args else {}\n        parser_args = dict(parser_args) if parser_args else {}\n        extra_payload = dict(extra) if extra else {}\n\n        if api_type == APITypes.RESPONSE:\n            return self._call_response_api(\n                dialog,\n                prompt,\n                model,\n                model_card,\n                client,\n                payload_args,\n                parser_args,\n                responder,\n                extra_payload,\n            )\n        return self._call_chat_api(\n            dialog,\n            prompt,\n            model,\n            model_card,\n            client,\n            payload_args,\n            parser_args,\n            responder,\n            extra_payload,\n        )\n\n    def stream(self, *args, **kwargs):\n        raise NotImplementedError(\"Streaming not yet implemented for OpenAIProvider\")\n\n    def _build_tools(self, prompt: Prompt) -&gt; List[Dict[str, Any]]:\n        tools: List[Dict[str, Any]] = []\n        for func in prompt.functions.values():\n            tool = func.to_tool(Providers.OPENAI)\n            if tool:\n                tools.append(tool)\n        for server in prompt.mcp_servers.values():\n            tool = server.to_tool(Providers.OPENAI)\n            if tool:\n                tools.append(tool)\n        return tools\n</code></pre>"},{"location":"reference/proxies/","title":"Proxies API Reference","text":""},{"location":"reference/proxies/#baseproxy","title":"BaseProxy","text":""},{"location":"reference/proxies/#lllm.proxies.base.BaseProxy","title":"<code>lllm.proxies.base.BaseProxy</code>","text":"<p>Base class for describing an API surface that agents can call as tools.</p> Source code in <code>lllm/proxies/base.py</code> <pre><code>class BaseProxy:\n    \"\"\"Base class for describing an API surface that agents can call as tools.\"\"\"\n\n    def __init__(\n        self,\n        *args,\n        activate_proxies: Optional[List[str]] = None,\n        cutoff_date: Optional[dt.datetime] = None,\n        deploy_mode: bool = False,\n        use_cache: bool = True,\n        auto_discover: Optional[bool] = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Support both legacy signatures (cutoff_date, use_cache) and the newer keyword-driven one.\n        \"\"\"\n        self.activate_proxies = activate_proxies[:] if activate_proxies else []\n        self.cutoff_date = cutoff_date\n        self.deploy_mode = deploy_mode\n        self.use_cache = use_cache\n        self.auto_discover = auto_discover\n\n        legacy_args = list(args)\n        if legacy_args:\n            first = legacy_args.pop(0)\n            if isinstance(first, list):  # legacy Proxy runtime order\n                self.activate_proxies = first\n                if legacy_args:\n                    self.cutoff_date = legacy_args.pop(0)\n                if legacy_args:\n                    self.deploy_mode = legacy_args.pop(0)\n            else:\n                self.cutoff_date = first\n                if legacy_args:\n                    self.use_cache = legacy_args.pop(0)\n\n        if isinstance(self.cutoff_date, str):\n            try:\n                self.cutoff_date = dt.datetime.fromisoformat(self.cutoff_date)\n            except ValueError:\n                self.cutoff_date = None\n\n    @staticmethod\n    def endpoint(category: str, endpoint: str, description: str, params: dict, response: list,\n                 name: str = None, sub_category: str = None, remove_keys: list = None,\n                 dt_cutoff: tuple = None, method: str = 'GET'):\n        \"\"\"Decorator that records metadata about an API endpoint.\"\"\"\n        def decorator(func):\n            func.endpoint_info = {\n                'category': category,\n                'endpoint': endpoint,\n                'name': name,\n                'description': description,\n                'sub_category': sub_category,\n                'remove_keys': remove_keys,\n                'params': params,\n                'response': response,\n                'dt_cutoff': dt_cutoff,\n                'method': method\n            }\n            return func\n        return decorator\n\n    @staticmethod\n    def postcall(func):\n        func.is_postcall = True\n        return func\n\n    # ------------------------------------------------------------------\n    # Endpoint metadata helpers\n    # ------------------------------------------------------------------\n\n    def _endpoint_methods(self):\n        \"\"\"\n        Yield ``(attr_name, method, endpoint_info)`` triples for every method\n        decorated with :func:`BaseProxy.endpoint`.\n        \"\"\"\n        for name, method in inspect.getmembers(self, predicate=callable):\n            info = getattr(method, \"endpoint_info\", None)\n            if info:\n                yield name, method, info\n\n    def endpoint_directory(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Return a structured list describing every endpoint exposed by this proxy.\n        \"\"\"\n        directory: List[Dict[str, Any]] = []\n        for name, method, info in self._endpoint_methods():\n            entry = dict(info)\n            entry.setdefault(\"name\", info.get(\"name\") or name)\n            entry[\"callable\"] = name\n            entry[\"docstring\"] = inspect.getdoc(method)\n            directory.append(entry)\n        directory.sort(key=lambda item: ((item.get(\"category\") or \"\"), item.get(\"endpoint\") or \"\"))\n        return directory\n\n    def api_directory(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Return proxy metadata plus the endpoint directory.\n        \"\"\"\n        return {\n            \"id\": getattr(self, \"_proxy_path\", self.__class__.__name__),\n            \"display_name\": getattr(self, \"_proxy_name\", self.__class__.__name__),\n            \"description\": getattr(self, \"_proxy_description\", \"\"),\n            \"endpoints\": self.endpoint_directory(),\n        }\n\n    def auto_test(self) -&gt; Dict[str, Dict[str, Any]]:\n        \"\"\"\n        Perform light-weight validation of endpoint metadata.\n\n        Returns a dict mapping callable name to ``{\"status\": \"...\", \"issues\": [...]}``.\n        \"\"\"\n        results: Dict[str, Dict[str, Any]] = {}\n        for entry in self.endpoint_directory():\n            issues: List[str] = []\n            params = entry.get(\"params\")\n            if not isinstance(params, dict):\n                issues.append(\"params\")\n            if entry.get(\"response\") is None:\n                issues.append(\"response\")\n            status = \"ok\" if not issues else \"warning\"\n            results[entry[\"callable\"]] = {\"status\": status, \"issues\": issues}\n        return results\n</code></pre>"},{"location":"reference/proxies/#lllm.proxies.base.BaseProxy.__init__","title":"<code>__init__(*args, activate_proxies=None, cutoff_date=None, deploy_mode=False, use_cache=True, auto_discover=None, **kwargs)</code>","text":"<p>Support both legacy signatures (cutoff_date, use_cache) and the newer keyword-driven one.</p> Source code in <code>lllm/proxies/base.py</code> <pre><code>def __init__(\n    self,\n    *args,\n    activate_proxies: Optional[List[str]] = None,\n    cutoff_date: Optional[dt.datetime] = None,\n    deploy_mode: bool = False,\n    use_cache: bool = True,\n    auto_discover: Optional[bool] = None,\n    **kwargs,\n):\n    \"\"\"\n    Support both legacy signatures (cutoff_date, use_cache) and the newer keyword-driven one.\n    \"\"\"\n    self.activate_proxies = activate_proxies[:] if activate_proxies else []\n    self.cutoff_date = cutoff_date\n    self.deploy_mode = deploy_mode\n    self.use_cache = use_cache\n    self.auto_discover = auto_discover\n\n    legacy_args = list(args)\n    if legacy_args:\n        first = legacy_args.pop(0)\n        if isinstance(first, list):  # legacy Proxy runtime order\n            self.activate_proxies = first\n            if legacy_args:\n                self.cutoff_date = legacy_args.pop(0)\n            if legacy_args:\n                self.deploy_mode = legacy_args.pop(0)\n        else:\n            self.cutoff_date = first\n            if legacy_args:\n                self.use_cache = legacy_args.pop(0)\n\n    if isinstance(self.cutoff_date, str):\n        try:\n            self.cutoff_date = dt.datetime.fromisoformat(self.cutoff_date)\n        except ValueError:\n            self.cutoff_date = None\n</code></pre>"},{"location":"reference/proxies/#lllm.proxies.base.BaseProxy.api_directory","title":"<code>api_directory()</code>","text":"<p>Return proxy metadata plus the endpoint directory.</p> Source code in <code>lllm/proxies/base.py</code> <pre><code>def api_directory(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Return proxy metadata plus the endpoint directory.\n    \"\"\"\n    return {\n        \"id\": getattr(self, \"_proxy_path\", self.__class__.__name__),\n        \"display_name\": getattr(self, \"_proxy_name\", self.__class__.__name__),\n        \"description\": getattr(self, \"_proxy_description\", \"\"),\n        \"endpoints\": self.endpoint_directory(),\n    }\n</code></pre>"},{"location":"reference/proxies/#lllm.proxies.base.BaseProxy.auto_test","title":"<code>auto_test()</code>","text":"<p>Perform light-weight validation of endpoint metadata.</p> <p>Returns a dict mapping callable name to <code>{\"status\": \"...\", \"issues\": [...]}</code>.</p> Source code in <code>lllm/proxies/base.py</code> <pre><code>def auto_test(self) -&gt; Dict[str, Dict[str, Any]]:\n    \"\"\"\n    Perform light-weight validation of endpoint metadata.\n\n    Returns a dict mapping callable name to ``{\"status\": \"...\", \"issues\": [...]}``.\n    \"\"\"\n    results: Dict[str, Dict[str, Any]] = {}\n    for entry in self.endpoint_directory():\n        issues: List[str] = []\n        params = entry.get(\"params\")\n        if not isinstance(params, dict):\n            issues.append(\"params\")\n        if entry.get(\"response\") is None:\n            issues.append(\"response\")\n        status = \"ok\" if not issues else \"warning\"\n        results[entry[\"callable\"]] = {\"status\": status, \"issues\": issues}\n    return results\n</code></pre>"},{"location":"reference/proxies/#lllm.proxies.base.BaseProxy.endpoint","title":"<code>endpoint(category, endpoint, description, params, response, name=None, sub_category=None, remove_keys=None, dt_cutoff=None, method='GET')</code>  <code>staticmethod</code>","text":"<p>Decorator that records metadata about an API endpoint.</p> Source code in <code>lllm/proxies/base.py</code> <pre><code>@staticmethod\ndef endpoint(category: str, endpoint: str, description: str, params: dict, response: list,\n             name: str = None, sub_category: str = None, remove_keys: list = None,\n             dt_cutoff: tuple = None, method: str = 'GET'):\n    \"\"\"Decorator that records metadata about an API endpoint.\"\"\"\n    def decorator(func):\n        func.endpoint_info = {\n            'category': category,\n            'endpoint': endpoint,\n            'name': name,\n            'description': description,\n            'sub_category': sub_category,\n            'remove_keys': remove_keys,\n            'params': params,\n            'response': response,\n            'dt_cutoff': dt_cutoff,\n            'method': method\n        }\n        return func\n    return decorator\n</code></pre>"},{"location":"reference/proxies/#lllm.proxies.base.BaseProxy.endpoint_directory","title":"<code>endpoint_directory()</code>","text":"<p>Return a structured list describing every endpoint exposed by this proxy.</p> Source code in <code>lllm/proxies/base.py</code> <pre><code>def endpoint_directory(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Return a structured list describing every endpoint exposed by this proxy.\n    \"\"\"\n    directory: List[Dict[str, Any]] = []\n    for name, method, info in self._endpoint_methods():\n        entry = dict(info)\n        entry.setdefault(\"name\", info.get(\"name\") or name)\n        entry[\"callable\"] = name\n        entry[\"docstring\"] = inspect.getdoc(method)\n        directory.append(entry)\n    directory.sort(key=lambda item: ((item.get(\"category\") or \"\"), item.get(\"endpoint\") or \"\"))\n    return directory\n</code></pre>"},{"location":"reference/proxies/#proxy","title":"Proxy","text":""},{"location":"reference/proxies/#lllm.proxies.base.Proxy","title":"<code>lllm.proxies.base.Proxy</code>","text":"<p>Runtime registry that instantiates every discovered proxy and forwards calls.</p> <p>Agents rarely instantiate this directly; instead the sandbox or higher-level tooling wires it up so prompts can enumerate available endpoints for tool selection.</p> Source code in <code>lllm/proxies/base.py</code> <pre><code>class Proxy:\n    \"\"\"\n    Runtime registry that instantiates every discovered proxy and forwards calls.\n\n    Agents rarely instantiate this directly; instead the sandbox or higher-level tooling\n    wires it up so prompts can enumerate available endpoints for tool selection.\n    \"\"\"\n\n    def __init__(\n        self,\n        activate_proxies: Optional[List[str]] = None,\n        cutoff_date: dt.datetime = None,\n        deploy_mode: bool = False,\n        *,\n        auto_discover: Optional[bool] = None,\n    ):\n        from lllm.core.discovery import auto_discover_if_enabled\n        auto_discover_if_enabled(auto_discover)\n        self.activate_proxies = activate_proxies or []\n        self.cutoff_date = cutoff_date\n        self.deploy_mode = deploy_mode\n        self.proxies: Dict[str, BaseProxy] = {}\n        self._auto_discover_flag = auto_discover\n        self._load_registered_proxies()\n\n    def _load_registered_proxies(self):\n        for name, proxy_cls in PROXY_REGISTRY.items():\n            if self.activate_proxies and name not in self.activate_proxies:\n                continue\n            try:\n                instance = proxy_cls(\n                    cutoff_date=self.cutoff_date,\n                    activate_proxies=self.activate_proxies,\n                    deploy_mode=self.deploy_mode,\n                    auto_discover=self._auto_discover_flag,\n                )\n            except TypeError:\n                # Fallback to legacy positional order if subclass has not been updated yet\n                instance = proxy_cls(self.activate_proxies, self.cutoff_date, self.deploy_mode)\n            self.proxies[name] = instance\n\n    def register(self, name: str, proxy_cls: Any):\n        \"\"\"Register (or override) a proxy implementation at runtime.\"\"\"\n        if name in self.proxies:\n            U.cprint(f'Proxy {name} already instantiated, overwriting instance', 'y')\n        try:\n            instance = proxy_cls(\n                cutoff_date=self.cutoff_date,\n                activate_proxies=self.activate_proxies,\n                deploy_mode=self.deploy_mode,\n                auto_discover=self._auto_discover_flag,\n            )\n        except TypeError:\n            instance = proxy_cls(self.activate_proxies, self.cutoff_date, self.deploy_mode)\n        self.proxies[name] = instance\n\n    def available(self) -&gt; List[str]:\n        \"\"\"Return the sorted list of proxy identifiers currently loaded.\"\"\"\n        return sorted(self.proxies.keys())\n\n    def api_catalog(self) -&gt; Dict[str, Dict[str, Any]]:\n        \"\"\"Return the API directory for every loaded proxy.\"\"\"\n        return {name: proxy.api_directory() for name, proxy in self.proxies.items()}\n\n    def get_api_directory(self, proxy_name: str) -&gt; Dict[str, Any]:\n        \"\"\"Convenience wrapper that returns the directory for a single proxy.\"\"\"\n        if proxy_name not in self.proxies:\n            raise KeyError(f\"Proxy '{proxy_name}' not registered\")\n        return self.proxies[proxy_name].api_directory()\n\n    def retrieve_api_docs(self, proxy_name: Optional[str] = None) -&gt; str:\n        \"\"\"\n        Render a human-readable overview of the available endpoints.\n\n        Args:\n            proxy_name: Optional identifier. If omitted, all proxies are included.\n        \"\"\"\n        if proxy_name is not None:\n            target_names = [proxy_name]\n        else:\n            target_names = sorted(self.proxies.keys())\n\n        sections: List[str] = []\n        for name in target_names:\n            if name not in self.proxies:\n                raise KeyError(f\"Proxy '{name}' not registered\")\n            meta = self.proxies[name].api_directory()\n            header = f\"## {meta['display_name']} ({name})\"\n            description = (meta.get(\"description\") or \"\").strip()\n            lines = [header]\n            if description:\n                lines.append(description)\n            for endpoint in meta.get(\"endpoints\", []):\n                method = (endpoint.get(\"method\") or \"GET\").upper()\n                desc = endpoint.get(\"description\") or \"\"\n                line = f\"- **{endpoint.get('endpoint')}** [{method}] \u2013 {desc}\"\n                lines.append(line.strip())\n                params = endpoint.get(\"params\") or {}\n                if isinstance(params, dict) and params:\n                    for param_name, spec in params.items():\n                        if isinstance(spec, tuple) and spec:\n                            type_hint = spec[0]\n                            example = spec[1] if len(spec) &gt; 1 else None\n                        else:\n                            type_hint = None\n                            example = spec\n                        type_name = getattr(type_hint, \"__name__\", str(type_hint)) if type_hint else \"Any\"\n                        if isinstance(example, (list, dict)):\n                            example_preview = str(example)[:60]\n                        else:\n                            example_preview = example\n                        lines.append(f\"    - `{param_name}` ({type_name}) e.g. {example_preview}\")\n            sections.append(\"\\n\".join(lines).strip())\n        return \"\\n\\n\".join(sections).strip()\n\n    def _resolve(self, endpoint: str) -&gt; tuple[str, str]:\n        if '.' in endpoint:\n            parts = endpoint.split('.', 1)\n            return parts[0], parts[1]\n        path_parts = endpoint.split('/')\n        if len(path_parts) &lt; 2:\n            raise ValueError(f\"Invalid endpoint '{endpoint}'. Use '&lt;proxy&gt;.&lt;method&gt;' or '&lt;proxy&gt;/&lt;method&gt;'.\")\n        return '/'.join(path_parts[:-1]), path_parts[-1]\n\n    def __call__(self, endpoint: str, *args, **kwargs):\n        \"\"\"Dispatch ``proxy_path.endpoint_name`` or ``proxy_path/endpoint`` to the proxy.\"\"\"\n        proxy_name, func_name = self._resolve(endpoint)\n        if proxy_name not in self.proxies:\n            raise KeyError(f\"Proxy '{proxy_name}' not registered. Available: {list(self.proxies.keys())}\")\n        proxy = self.proxies[proxy_name]\n        if not hasattr(proxy, func_name):\n            raise AttributeError(f\"Proxy '{proxy_name}' has no endpoint '{func_name}'\")\n        handler = getattr(proxy, func_name)\n        return handler(*args, **kwargs)\n</code></pre>"},{"location":"reference/proxies/#lllm.proxies.base.Proxy.__call__","title":"<code>__call__(endpoint, *args, **kwargs)</code>","text":"<p>Dispatch <code>proxy_path.endpoint_name</code> or <code>proxy_path/endpoint</code> to the proxy.</p> Source code in <code>lllm/proxies/base.py</code> <pre><code>def __call__(self, endpoint: str, *args, **kwargs):\n    \"\"\"Dispatch ``proxy_path.endpoint_name`` or ``proxy_path/endpoint`` to the proxy.\"\"\"\n    proxy_name, func_name = self._resolve(endpoint)\n    if proxy_name not in self.proxies:\n        raise KeyError(f\"Proxy '{proxy_name}' not registered. Available: {list(self.proxies.keys())}\")\n    proxy = self.proxies[proxy_name]\n    if not hasattr(proxy, func_name):\n        raise AttributeError(f\"Proxy '{proxy_name}' has no endpoint '{func_name}'\")\n    handler = getattr(proxy, func_name)\n    return handler(*args, **kwargs)\n</code></pre>"},{"location":"reference/proxies/#lllm.proxies.base.Proxy.api_catalog","title":"<code>api_catalog()</code>","text":"<p>Return the API directory for every loaded proxy.</p> Source code in <code>lllm/proxies/base.py</code> <pre><code>def api_catalog(self) -&gt; Dict[str, Dict[str, Any]]:\n    \"\"\"Return the API directory for every loaded proxy.\"\"\"\n    return {name: proxy.api_directory() for name, proxy in self.proxies.items()}\n</code></pre>"},{"location":"reference/proxies/#lllm.proxies.base.Proxy.available","title":"<code>available()</code>","text":"<p>Return the sorted list of proxy identifiers currently loaded.</p> Source code in <code>lllm/proxies/base.py</code> <pre><code>def available(self) -&gt; List[str]:\n    \"\"\"Return the sorted list of proxy identifiers currently loaded.\"\"\"\n    return sorted(self.proxies.keys())\n</code></pre>"},{"location":"reference/proxies/#lllm.proxies.base.Proxy.get_api_directory","title":"<code>get_api_directory(proxy_name)</code>","text":"<p>Convenience wrapper that returns the directory for a single proxy.</p> Source code in <code>lllm/proxies/base.py</code> <pre><code>def get_api_directory(self, proxy_name: str) -&gt; Dict[str, Any]:\n    \"\"\"Convenience wrapper that returns the directory for a single proxy.\"\"\"\n    if proxy_name not in self.proxies:\n        raise KeyError(f\"Proxy '{proxy_name}' not registered\")\n    return self.proxies[proxy_name].api_directory()\n</code></pre>"},{"location":"reference/proxies/#lllm.proxies.base.Proxy.register","title":"<code>register(name, proxy_cls)</code>","text":"<p>Register (or override) a proxy implementation at runtime.</p> Source code in <code>lllm/proxies/base.py</code> <pre><code>def register(self, name: str, proxy_cls: Any):\n    \"\"\"Register (or override) a proxy implementation at runtime.\"\"\"\n    if name in self.proxies:\n        U.cprint(f'Proxy {name} already instantiated, overwriting instance', 'y')\n    try:\n        instance = proxy_cls(\n            cutoff_date=self.cutoff_date,\n            activate_proxies=self.activate_proxies,\n            deploy_mode=self.deploy_mode,\n            auto_discover=self._auto_discover_flag,\n        )\n    except TypeError:\n        instance = proxy_cls(self.activate_proxies, self.cutoff_date, self.deploy_mode)\n    self.proxies[name] = instance\n</code></pre>"},{"location":"reference/proxies/#lllm.proxies.base.Proxy.retrieve_api_docs","title":"<code>retrieve_api_docs(proxy_name=None)</code>","text":"<p>Render a human-readable overview of the available endpoints.</p> <p>Parameters:</p> Name Type Description Default <code>proxy_name</code> <code>Optional[str]</code> <p>Optional identifier. If omitted, all proxies are included.</p> <code>None</code> Source code in <code>lllm/proxies/base.py</code> <pre><code>def retrieve_api_docs(self, proxy_name: Optional[str] = None) -&gt; str:\n    \"\"\"\n    Render a human-readable overview of the available endpoints.\n\n    Args:\n        proxy_name: Optional identifier. If omitted, all proxies are included.\n    \"\"\"\n    if proxy_name is not None:\n        target_names = [proxy_name]\n    else:\n        target_names = sorted(self.proxies.keys())\n\n    sections: List[str] = []\n    for name in target_names:\n        if name not in self.proxies:\n            raise KeyError(f\"Proxy '{name}' not registered\")\n        meta = self.proxies[name].api_directory()\n        header = f\"## {meta['display_name']} ({name})\"\n        description = (meta.get(\"description\") or \"\").strip()\n        lines = [header]\n        if description:\n            lines.append(description)\n        for endpoint in meta.get(\"endpoints\", []):\n            method = (endpoint.get(\"method\") or \"GET\").upper()\n            desc = endpoint.get(\"description\") or \"\"\n            line = f\"- **{endpoint.get('endpoint')}** [{method}] \u2013 {desc}\"\n            lines.append(line.strip())\n            params = endpoint.get(\"params\") or {}\n            if isinstance(params, dict) and params:\n                for param_name, spec in params.items():\n                    if isinstance(spec, tuple) and spec:\n                        type_hint = spec[0]\n                        example = spec[1] if len(spec) &gt; 1 else None\n                    else:\n                        type_hint = None\n                        example = spec\n                    type_name = getattr(type_hint, \"__name__\", str(type_hint)) if type_hint else \"Any\"\n                    if isinstance(example, (list, dict)):\n                        example_preview = str(example)[:60]\n                    else:\n                        example_preview = example\n                    lines.append(f\"    - `{param_name}` ({type_name}) e.g. {example_preview}\")\n        sections.append(\"\\n\".join(lines).strip())\n    return \"\\n\\n\".join(sections).strip()\n</code></pre>"},{"location":"reference/proxies/#proxyregistrator","title":"ProxyRegistrator","text":""},{"location":"reference/proxies/#lllm.proxies.base.ProxyRegistrator","title":"<code>lllm.proxies.base.ProxyRegistrator(path, name, description)</code>","text":"Source code in <code>lllm/proxies/base.py</code> <pre><code>def ProxyRegistrator(path: str, name: str, description: str):\n    def decorator(cls):\n        cls._proxy_path = path\n        cls._proxy_name = name\n        cls._proxy_description = description\n        register_proxy(path, cls, overwrite=True)\n        return cls\n    return decorator\n</code></pre>"}]}